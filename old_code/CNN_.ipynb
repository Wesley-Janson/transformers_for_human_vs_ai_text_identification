{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wesley-Janson/transformers_for_human_vs_ai_text_identification/blob/main/CNN_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sVt0Q2rzlJJs"
      },
      "outputs": [],
      "source": [
        "# Import relevant packages and data_loader.py\n",
        "#import data_loader\n",
        "\n",
        "# importing the libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# for reading and displaying images\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# for creating validation set\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# for evaluating the model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# PyTorch libraries and modules\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv1d, Conv2d, MaxPool1d, MaxPool2d, Module, Softmax, BatchNorm1d, BatchNorm2d, Dropout\n",
        "from torch.optim import Adam, SGD\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "k8CDKMYwpjwm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aRaZIij8puzX"
      },
      "outputs": [],
      "source": [
        "def load_data(csv):\n",
        "  # Reads the raw csv file and split into\n",
        "  # sentences (x) and target (y)\n",
        "  df = pd.read_csv(csv)  \n",
        "  text = df['intro'].values\n",
        "  labels = df['type'].values\n",
        "  return labels,text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oIY1CJJ5p0zA"
      },
      "outputs": [],
      "source": [
        "\n",
        "# This function processes training data, establishing number IDs for each vocabulary word,\n",
        "# converting word sequence into ID sequence (input_as_ids), and providing dict\n",
        "# to map from word to its ID (word2id), and list to map from ID back to word (id2word)\n",
        "def process_training_data(corpus_text):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        # Create the model's vocabulary and map to unique indices\n",
        "        word2id = {}\n",
        "        id2word = []\n",
        "        indexes_dropped = []\n",
        "        list_of_inputs = []\n",
        "        for j, entry in enumerate(corpus_text):\n",
        "            for i,word in enumerate(entry):\n",
        "                if 7<i<=30:\n",
        "                    if word not in word2id:\n",
        "                        id2word.append(word)\n",
        "                        word2id[word] = len(id2word) - 1\n",
        "\n",
        "            # Convert string of text into string of IDs in a tensor for input to model\n",
        "            input_as_ids = []\n",
        "            for i,word in enumerate(entry):\n",
        "                if 7<i<=30:\n",
        "                    input_as_ids.append(word2id[word])\n",
        "            if len(input_as_ids) == 23:\n",
        "              list_of_inputs.append(input_as_ids)\n",
        "            else:\n",
        "              indexes_dropped.append(j)\n",
        "            # final_ids = torch.LongTensor(input_as_ids)\n",
        "        list_of_inputs = torch.Tensor(list_of_inputs)\n",
        "\n",
        "        return list_of_inputs,word2id,id2word, indexes_dropped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 307,
      "metadata": {
        "id": "5T4irZ27ls7Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7d45611-8366-4785-b18e-0140ce20c51b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "240000\n"
          ]
        }
      ],
      "source": [
        "# Run data loader\n",
        "labels, text = load_data('data.csv')\n",
        "small_labels = labels[:1000]\n",
        "small_text = text[:1000]\n",
        "train_x, val_x, train_y, val_y = train_test_split(text, labels, test_size = 0.2)\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "print(len(train_x))\n",
        "list_of_tokens_train = [tokenizer(x) for x in train_x]\n",
        "list_of_tokens_val = [tokenizer(x) for x in val_x]\n",
        "\n",
        "train_x,word2id,id2word, indexes_dropped_train = process_training_data(list_of_tokens_train)\n",
        "\n",
        "val_x,word2id,id2word, indexes_dropped_val = process_training_data(list_of_tokens_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_train = [x for x in indexes_dropped_train]\n",
        "new_val = [x for x in indexes_dropped_val]\n",
        "train_y = list(train_y)\n",
        "for index, element in enumerate(new_train):\n",
        "  train_y = train_y[:indexes_dropped_train[index]] + train_y[indexes_dropped_train[index]+1:]\n",
        "  for i in range(len(indexes_dropped_train)):\n",
        "    indexes_dropped_train[i] -= 1\n",
        "for index, element in enumerate(new_val):\n",
        "  val_y = list(val_y[:indexes_dropped_val[index]]) + list(val_y[indexes_dropped_val[index]+1:])\n",
        "  for i in range(len(indexes_dropped_val)):\n",
        "    indexes_dropped_train[i] -= 1\n",
        "train_y = np.asarray(train_y)\n",
        "val_y = np.asarray(val_y)\n",
        "train_y = torch.Tensor(train_y.reshape((len(train_y), 1)))\n",
        "val_y = torch.Tensor(val_y.reshape((len(val_y), 1)))\n",
        "print(train_x.shape)\n",
        "print(train_y.shape)\n",
        "print(val_x.shape)\n",
        "print(val_y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEao8vNKIceW",
        "outputId": "474ed8c7-c50f-423f-a0b9-3c01f7189101"
      },
      "execution_count": 308,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([238935, 23])\n",
            "torch.Size([238935, 1])\n",
            "torch.Size([59710, 23])\n",
            "torch.Size([59710, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 309,
      "metadata": {
        "id": "qZds-vUYwdW3"
      },
      "outputs": [],
      "source": [
        "# from torchtext.data.utils import get_tokenizer\n",
        "# from torchtext.vocab import build_vocab_from_iterator\n",
        "# labels,text = load_data('data.csv')\n",
        "# train_x, val_x, train_y, val_y = train_test_split(text, labels, test_size = 0.8)\n",
        "\n",
        "# tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "# def yield_tokens(data_iter):\n",
        "#     for text in data_iter:\n",
        "#         yield tokenizer(text)\n",
        "\n",
        "# vocab = build_vocab_from_iterator(yield_tokens(train_x), specials=[\"<unk>\"])\n",
        "# vocab.set_default_index(vocab[\"<unk>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 310,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omleOMTjwJ3S",
        "outputId": "fd595329-7b5c-4e99-8bb2-5b857d70786c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(238935, 24)\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "text_pipeline = lambda x: x\n",
        "label_pipeline = lambda x: int(x)\n",
        "train_both = np.concatenate([train_x,train_y], axis = 1)\n",
        "val_both = np.concatenate([val_x,val_y], axis = 1)\n",
        "print(train_both.shape)\n",
        "print(type(train_both))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qX7ujgYO8g7-"
      },
      "execution_count": 310,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 311,
      "metadata": {
        "id": "6zQmxBERv2lg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cd22c0a-fd50-478e-ada9-a87dcfc3b485"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23893 (tensor([1, 0, 1, 1, 0]), tensor([    24,      4,     46,    684,     22,     13,    124,     12,     53,\n",
            "            10,    192,     22,   1415,     24,    149,      3,     11,     12,\n",
            "            13,     89,   1949,    179,     13,    601,    409,      2,      3,\n",
            "            96,      7,  53363,      6,    536,     12,     13,  67780,  17682,\n",
            "            10,   3894,     45,     13, 182823,   3203,     24,     26,      3,\n",
            "            15,    808,    285,     70,    708,     22,   4843,     24,   3018,\n",
            "             6,    284,   2787,     10,   6283, 182824,      3,     21,     22,\n",
            "           182,     22,     13,    823,     12,  13650,   3030,      2,     40,\n",
            "             4,   2591,    138,    631,   2212,   2213,    657,      6,   1428,\n",
            "          1941,   5092,     12,     13,   2599,   2600,  17286,     12,     13,\n",
            "            75,   2213,      3,      4,   1729,  26917,      6,  17611,     10,\n",
            "            22,   7699,     24,     26,   2354,     13,     62,    550,    551,\n",
            "           246,   2640,    243,   1393,    429,     26,   6718]), tensor([ 0, 23, 46, 69, 92]))\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for entry in batch:\n",
        "         _label = entry[-1] \n",
        "         _text = entry[:len(entry)-1] \n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
        "batch_size_var = 10\n",
        "train_loader = DataLoader(train_both, batch_size=batch_size_var, shuffle=False, collate_fn=collate_batch)\n",
        "val_loader = DataLoader(train_both, batch_size=batch_size_var, shuffle=False, collate_fn=collate_batch)\n",
        "trainSteps = len(train_loader.dataset) // batch_size_var\n",
        "valSteps = len(val_loader.dataset) // batch_size_var\n",
        "for i, batch in enumerate(train_loader):\n",
        "  a,b,c = batch\n",
        "  if len(a) != 10:\n",
        "      print(i, batch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 312,
      "metadata": {
        "id": "CavGDuyJxt5l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1792872-d599-4853-ebd0-4a6399008101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1])\n"
          ]
        }
      ],
      "source": [
        "next_, labels_, _offset = next(iter(train_loader))\n",
        "print(next_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 313,
      "metadata": {
        "id": "nv3_bNOlltWp"
      },
      "outputs": [],
      "source": [
        "# Define Architecture\n",
        "\n",
        "class CNN(Module):   \n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.cnn_layers = Sequential(\n",
        "            # Defining a 2D convolution layer\n",
        "            Conv1d(10, 4000, kernel_size=3, stride=1, padding=1),\n",
        "            BatchNorm1d(23),\n",
        "            ReLU(inplace=True),\n",
        "            MaxPool1d(kernel_size=2, stride=2),\n",
        "            # Defining another 2D convolution layer\n",
        "            Conv1d(4000, 10, kernel_size=3, stride=1, padding=1),\n",
        "            BatchNorm1d(11),\n",
        "            ReLU(inplace=True),\n",
        "            MaxPool1d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.linear_layers = Sequential(\n",
        "            Linear(5, 1)\n",
        "        )\n",
        "\n",
        "    # Defining the forward pass    \n",
        "    def forward(self, x):\n",
        "        x = self.cnn_layers(x.float())\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear_layers(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 314,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQJ_G32eluyN",
        "outputId": "e045fd01-bbe1-453c-a368-e3b8dcb3182d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN(\n",
            "  (cnn_layers): Sequential(\n",
            "    (0): Conv1d(10, 4000, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "    (1): BatchNorm1d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv1d(4000, 10, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "    (5): BatchNorm1d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (linear_layers): Sequential(\n",
            "    (0): Linear(in_features=5, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# defining the model\n",
        "model = CNN()\n",
        "# defining the optimizer\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "# defining the loss function\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "# checking if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    criterion = criterion.cuda()\n",
        "    \n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#logistic regression bag of words, can get fwearture importance\n",
        "\n",
        "#how get feature importance in CNN's\n",
        "\n",
        "#how get tokesn out of featurs\n",
        "\n",
        "#run examples we know, print which filter getting trigegred, associate words or grams with filteere\n",
        "\n",
        "#tsney (visualize nerual net)\n",
        "\n",
        "#take examples human got wrong, see what we get right, find k-grams that ar enon-triavail"
      ],
      "metadata": {
        "id": "7lv7xQRCx1ln"
      },
      "execution_count": 315,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 316,
      "metadata": {
        "id": "J6z3lxJflwmH"
      },
      "outputs": [],
      "source": [
        "# Create train function\n",
        "def train_epoch(epoch):\n",
        "    running_loss = 0\n",
        "    last_loss = 0\n",
        "\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "    for i, data in enumerate(train_loader):\n",
        "\n",
        "        # Every data instance is an input + label pair\n",
        "        labels, inputs, offset = data\n",
        "        if len(labels) == 10:\n",
        "          # Zero your gradients for every batch!\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Make predictions for this batch\n",
        "          inputs = inputs.reshape([10, 23])\n",
        "          labels = labels.reshape([10, 1])\n",
        "          inputs = inputs.to(device=device,dtype=torch.float64) \n",
        "          labels = labels.to(device=device, dtype=torch.float64)   \n",
        "          \n",
        "          outputs = model(inputs)\n",
        "          # Compute the loss and its gradients\n",
        "          loss = criterion(outputs.squeeze(1), labels.squeeze(1))\n",
        "          loss.backward()\n",
        "\n",
        "          # Adjust learning weights\n",
        "          optimizer.step()\n",
        "\n",
        "          # Gather data and report\n",
        "          running_loss += loss.item()\n",
        "          if i % 10 == 0:\n",
        "              last_loss = running_loss / 10 # loss per batch\n",
        "              running_loss = 0\n",
        "              #print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "        else:\n",
        "          pass\n",
        "    print(f'Epoch: {epoch}, Loss: {last_loss}')\n",
        "    return last_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN()\n",
        "# defining the optimizer\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "# defining the loss function\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "# checking if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    criterion = criterion.cuda()\n",
        "\n",
        "epoch_number = 0\n",
        "\n",
        "EPOCHS = 20\n",
        "\n",
        "best_vloss = 10000000.\n",
        "train_losses = [] \n",
        "val_losses = []\n",
        "for epoch in range(EPOCHS):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "    # Make sure gradient tracking is on, and do a pass over the data\n",
        "    model.train(True)\n",
        "    avg_loss = train_epoch(epoch_number)\n",
        "    train_losses.append(avg_loss)\n",
        "\n",
        "    # We don't need gradients on to do reporting\n",
        "    model.train(False)\n",
        "\n",
        "    running_vloss = 0.0\n",
        "    for i, vdata in enumerate(val_loader):\n",
        "        vlabels, vinputs, offset = vdata\n",
        "        if len(vlabels) == 10:\n",
        "          vinputs = vinputs.reshape([10, 23])\n",
        "          vlabels = vlabels.reshape([10, 1])\n",
        "          vlabels = vlabels.type(torch.LongTensor)\n",
        "          vinputs = vinputs.to(device=device,dtype=torch.float64) \n",
        "          vlabels = vlabels.to(device=device, dtype=torch.float64)   \n",
        "          voutputs = model(vinputs)\n",
        "          vloss = criterion(voutputs.squeeze(1), vlabels.squeeze(1))\n",
        "          running_vloss += vloss\n",
        "        else:\n",
        "          pass\n",
        "    avg_vloss = running_vloss /len(val_loader)\n",
        "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
        "\n",
        "    val_losses.append(avg_loss)\n",
        "    # Track best performance, and save the model's state\n",
        "    if avg_vloss < best_vloss:\n",
        "        best_vloss = avg_vloss\n",
        "\n",
        "    epoch_number += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWUhv7b-WJMk",
        "outputId": "d7117b05-565f-4273-e9ca-2ecc758a45e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1:\n",
            "Epoch: 0, Loss: 0.6718283584341406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.pylab import plt\n",
        "from numpy import arange\n",
        " \n",
        "# Load the training and validation loss dictionaries\n",
        "print(len(train_losses))\n",
        "print(len(val_losses))\n",
        "# Generate a sequence of integers to represent the epoch numbers\n",
        "epochs = range(0, EPOCHS)\n",
        " \n",
        "# Plot and label the training and validation loss values\n",
        "plt.plot(epochs, train_losses, label='Training Loss')\n",
        "plt.plot(epochs, val_losses, label='Validation Loss')\n",
        " \n",
        "# Add in a title and axes labels\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        " \n",
        "# Set the tick locations\n",
        "plt.xticks(arange(0, EPOCHS, 2))\n",
        " \n",
        "# Display the plot\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JuP30WH9Zy0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Aabx7pCHZyyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sS0XqOnQZyt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5sYi_ZcbZysN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Z8siSO-ZyqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vO86JbChZyoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K6XBwaZNZymS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xVDaFmvTZykO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l79J19TCZyhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # model.train()\n",
        "    # tr_loss = 0\n",
        "    # # getting the training set\n",
        "    # print(train_x)\n",
        "    # print(train_y)\n",
        "    # print(train_x.shape)\n",
        "    # print(train_y.shape)\n",
        "    # x_train, y_train = Variable(torch.Tensor(train_x)), Variable(torch.Tensor(train_y))\n",
        "    # # getting the validation set\n",
        "    # x_val, y_val = Variable(val_x), Variable(val_y)\n",
        "    # # converting the data into GPU format\n",
        "    # if torch.cuda.is_available():\n",
        "    #     x_train = x_train.cuda()\n",
        "    #     y_train = y_train.cuda()\n",
        "    #     x_val = x_val.cuda()\n",
        "    #     y_val = y_val.cuda()\n",
        "\n",
        "    # # clearing the Gradients of the model parameters\n",
        "    # optimizer.zero_grad()\n",
        "    \n",
        "    # # prediction for training and validation set\n",
        "    # output_train = model(x_train)\n",
        "    # output_val = model(x_val)\n",
        "\n",
        "    # # computing the training and validation loss\n",
        "    # loss_train = criterion(output_train, y_train)\n",
        "    # loss_val = criterion(output_val, y_val)\n",
        "    # train_losses.append(loss_train)\n",
        "    # val_losses.append(loss_val)\n",
        "\n",
        "    # # computing the updated weights of all the model parameters\n",
        "    # loss_train.backward()\n",
        "    # optimizer.step()\n",
        "    # tr_loss = loss_train.item()\n",
        "    # if epoch%2 == 0:\n",
        "    #     # printing the validation loss\n",
        "    #     print('Epoch : ',epoch+1, '\\t', 'loss :', loss_val)"
      ],
      "metadata": {
        "id": "vamkSejNZyYq"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "Do_4PK5Klx1_",
        "outputId": "b46e59bb-5225-4850-db5b-95a400ecc86c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-a83c0b7cb81c>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# training the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ],
      "source": [
        "# defining the number of epochs\n",
        "n_epochs = 25\n",
        "# empty list to store training losses\n",
        "train_losses = []\n",
        "# empty list to store validation losses\n",
        "val_losses = []\n",
        "# training the model\n",
        "for epoch in range(n_epochs):\n",
        "  train(epoch)\n",
        "print(train_losses)\n",
        "print(val_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTk8jOI-reYo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1uYL8qjFTmqSwUOiGG1V4Nuut6-puTFf5",
      "authorship_tag": "ABX9TyOxbM/W+cLLs66WnsTlfOzD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}