{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wesley-Janson/transformers_for_human_vs_ai_text_identification/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "sVt0Q2rzlJJs"
      },
      "outputs": [],
      "source": [
        "# Import relevant packages and data_loader.py\n",
        "#import data_loader\n",
        "\n",
        "# importing the libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# for reading and displaying images\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# for creating validation set\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# for evaluating the model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# PyTorch libraries and modules\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv1d, Conv2d, MaxPool1d, MaxPool2d, Module, Softmax, BatchNorm1d, BatchNorm2d, Dropout, Embedding\n",
        "from torch.optim import Adam, SGD\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "k8CDKMYwpjwm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "aRaZIij8puzX"
      },
      "outputs": [],
      "source": [
        "def load_data(csv):\n",
        "  # Reads the raw csv file and split into\n",
        "  # sentences (x) and target (y)\n",
        "  df = pd.read_csv(csv)  \n",
        "  text = df['intro'].values\n",
        "  labels = df['type'].values\n",
        "  return labels,text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "oIY1CJJ5p0zA"
      },
      "outputs": [],
      "source": [
        "\n",
        "# This function processes training data, establishing number IDs for each vocabulary word,\n",
        "# converting word sequence into ID sequence (input_as_ids), and providing dict\n",
        "# to map from word to its ID (word2id), and list to map from ID back to word (id2word)\n",
        "def process_training_data(corpus_text):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        # Create the model's vocabulary and map to unique indices\n",
        "        word2id = {}\n",
        "        id2word = []\n",
        "        indexes_dropped = []\n",
        "        list_of_inputs = []\n",
        "        for j, entry in enumerate(corpus_text):\n",
        "            for i,word in enumerate(entry):\n",
        "                if 7<i<=30:\n",
        "                    if word not in word2id:\n",
        "                        id2word.append(word)\n",
        "                        word2id[word] = len(id2word) - 1\n",
        "\n",
        "            # Convert string of text into string of IDs in a tensor for input to model\n",
        "            input_as_ids = []\n",
        "            for i,word in enumerate(entry):\n",
        "                if 7<i<=30:\n",
        "                    input_as_ids.append(word2id[word])\n",
        "            if len(input_as_ids) == 23:\n",
        "              list_of_inputs.append(input_as_ids)\n",
        "            else:\n",
        "              indexes_dropped.append(j)\n",
        "            # final_ids = torch.LongTensor(input_as_ids)\n",
        "        list_of_inputs = torch.Tensor(list_of_inputs)\n",
        "\n",
        "        return list_of_inputs,word2id,id2word, indexes_dropped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5T4irZ27ls7Z",
        "outputId": "c7e3d556-3533-4122-f0a9-2f817c8d304c"
      },
      "outputs": [],
      "source": [
        "# Run data loader\n",
        "labels, text = load_data('data/data.csv')\n",
        "train_x, val_x, train_y, val_y = train_test_split(text, labels, test_size = 0.8)\n",
        "val_x, test_x, val_y, test_y = train_test_split(val_x, val_y, test_size = 0.5)\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "list_of_tokens_train = [tokenizer(x) for x in train_x]\n",
        "list_of_tokens_val = [tokenizer(x) for x in val_x]\n",
        "list_of_tokens_test = [tokenizer(x) for x in test_x]\n",
        "\n",
        "train_x,word2id_train,id2word_train, indexes_dropped_train = process_training_data(list_of_tokens_train)\n",
        "\n",
        "val_x,word2id_val,id2word_val, indexes_dropped_val = process_training_data(list_of_tokens_val)\n",
        "\n",
        "test_x,word2id_test,id2word_test, indexes_dropped_test = process_training_data(list_of_tokens_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEao8vNKIceW",
        "outputId": "79e73750-7a42-4dd5-c535-740bb1bbd698"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([59711, 23])\n",
            "torch.Size([59711, 1])\n",
            "torch.Size([119454, 23])\n",
            "torch.Size([119454, 1])\n",
            "torch.Size([119480, 23])\n",
            "torch.Size([119480, 1])\n",
            "(59711, 24)\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "new_train = [x for x in indexes_dropped_train]\n",
        "new_val = [x for x in indexes_dropped_val]\n",
        "new_test = [x for x in indexes_dropped_test]\n",
        "\n",
        "train_y = list(train_y)\n",
        "for index, element in enumerate(new_train):\n",
        "    del train_y[element - index]  # Remove element from train_y\n",
        "    for i in range(len(indexes_dropped_train)):\n",
        "        if indexes_dropped_train[i] > element:\n",
        "            indexes_dropped_train[i] -= 1  # Decrement indexes after removal\n",
        "\n",
        "val_y = list(val_y)\n",
        "for index, element in enumerate(new_val):\n",
        "    del val_y[element - index]  # Remove element from val_y\n",
        "    for i in range(len(indexes_dropped_val)):\n",
        "        if indexes_dropped_val[i] > element:\n",
        "            indexes_dropped_val[i] -= 1  # Decrement indexes after removal\n",
        "\n",
        "test_y = list(test_y)\n",
        "for index, element in enumerate(new_test):\n",
        "    del test_y[element - index]  # Remove element from test_y\n",
        "    for i in range(len(indexes_dropped_test)):\n",
        "        if indexes_dropped_test[i] > element:\n",
        "            indexes_dropped_test[i] -= 1  # Decrement indexes after removal\n",
        "\n",
        "train_y = np.asarray(train_y)\n",
        "val_y = np.asarray(val_y)\n",
        "train_y = torch.Tensor(train_y.reshape((len(train_y), 1)))\n",
        "val_y = torch.Tensor(val_y.reshape((len(val_y), 1)))\n",
        "test_y = np.asarray(test_y)\n",
        "test_y = torch.Tensor(test_y.reshape((len(test_y), 1)))\n",
        "\n",
        "# Check the shapes of the arrays after the modifications\n",
        "print(train_x.shape)\n",
        "print(train_y.shape)\n",
        "print(val_x.shape)\n",
        "print(val_y.shape)\n",
        "print(test_x.shape)\n",
        "print(test_y.shape)\n",
        "\n",
        "text_pipeline = lambda x: x\n",
        "label_pipeline = lambda x: int(x)\n",
        "train_both = np.concatenate([train_x, train_y], axis=1)\n",
        "val_both = np.concatenate([val_x, val_y], axis=1)\n",
        "test_both = np.concatenate([test_x, test_y], axis=1)\n",
        "print(train_both.shape)\n",
        "print(type(train_both))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zQmxBERv2lg",
        "outputId": "9adc0bb9-af59-4606-f41e-b7a57d360ddc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5971 (tensor([1]), tensor([78098,    53,    97,    64,    45,  2493, 10851,   754,     0,     1,\n",
            "         3926,    14,    43,    18,    82,    33,  1396,  1441,    27,   426,\n",
            "          152,   188,  4615]), tensor([0]))\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for entry in batch:\n",
        "         _label = entry[-1] \n",
        "         _text = entry[:len(entry)-1] \n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
        "batch_size_var = 10\n",
        "train_loader = DataLoader(train_both, batch_size=batch_size_var, shuffle=False, collate_fn=collate_batch)\n",
        "val_loader = DataLoader(train_both, batch_size=batch_size_var, shuffle=False, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_both, batch_size=batch_size_var, shuffle=False, collate_fn=collate_batch)\n",
        "trainSteps = len(train_loader.dataset) // batch_size_var\n",
        "valSteps = len(val_loader.dataset) // batch_size_var\n",
        "testSteps = len(test_loader.dataset) // batch_size_var\n",
        "for i, batch in enumerate(train_loader):\n",
        "  a,b,c = batch\n",
        "  if len(a) != 10:\n",
        "      print(i, batch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CavGDuyJxt5l",
        "outputId": "c270404b-ba35-44ff-b9d1-c6088a4bcf22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0])\n"
          ]
        }
      ],
      "source": [
        "next_, labels_, _offset = next(iter(train_loader))\n",
        "print(next_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=256, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=64, kernel_size=3, padding=1)\n",
        "\n",
        "        # Max pooling layer\n",
        "        self.max_pool = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(128, 1)  # Assuming the output size is 2\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        embedded = embedded.permute(0, 2, 1)\n",
        "        x = self.conv1(embedded)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.max_pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.max_pool(x)\n",
        "        x = self.conv3(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.max_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN(\n",
            "  (embedding): Embedding(78099, 23)\n",
            "  (conv1): Conv1d(23, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (conv2): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (conv3): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (max_pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# defining the model\n",
        "vocab_size = len(word2id_train)\n",
        "embedding_dim = 23\n",
        "\n",
        "model = CNN(vocab_size, embedding_dim)\n",
        "# defining the optimizer\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "# defining the loss function\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "# checking if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    criterion = criterion.cuda()\n",
        "    \n",
        "print(model)\n",
        "\n",
        "def compute_metrics(predictions, targets):\n",
        "    # Round predictions to get binary values\n",
        "    rounded_preds = torch.round(torch.sigmoid(predictions))\n",
        "\n",
        "    # Calculate true positives, false positives, and false negatives\n",
        "    true_positives = torch.logical_and(rounded_preds == 1, targets == 1).sum().item()\n",
        "    false_positives = torch.logical_and(rounded_preds == 1, targets == 0).sum().item()\n",
        "    false_negatives = torch.logical_and(rounded_preds == 0, targets == 1).sum().item()\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = torch.eq(rounded_preds, targets).sum().item() / targets.size(0)\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    if true_positives + false_positives > 0:\n",
        "        precision = true_positives / (true_positives + false_positives)\n",
        "    else:\n",
        "        precision = 0.0\n",
        "\n",
        "    recall = true_positives / (true_positives + false_negatives)\n",
        "\n",
        "    return accuracy, precision, recall\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "7lv7xQRCx1ln"
      },
      "outputs": [],
      "source": [
        "#logistic regression bag of words, can get fwearture importance\n",
        "\n",
        "#how get feature importance in CNN's\n",
        "\n",
        "#how get tokesn out of featurs\n",
        "\n",
        "#run examples we know, print which filter getting trigegred, associate words or grams with filteere\n",
        "\n",
        "#tsney (visualize nerual net)\n",
        "\n",
        "#take examples human got wrong, see what we get right, find k-grams that ar enon-triavail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_accuracies = []\n",
        "train_precisions = []\n",
        "train_recalls = []\n",
        "val_accuracies = []\n",
        "val_precisions = []\n",
        "val_recalls = []\n",
        "test_accuracies = []\n",
        "test_precisions = []\n",
        "test_recalls = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(epoch):\n",
        "    running_loss = 0.0\n",
        "    total_predictions = []\n",
        "    total_labels = []\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "        labels, inputs, offset = data\n",
        "        if len(labels) == 10:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            inputs = inputs.reshape([10, 23]).to(device=device, dtype=torch.long)\n",
        "            labels = labels.float().to(device=device)  # Convert labels to float\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            loss = criterion(outputs, labels.unsqueeze(1))\n",
        "\n",
        "            # Calculate metrics\n",
        "            rounded_preds = torch.round(torch.sigmoid(outputs))\n",
        "            total_predictions.extend(rounded_preds.tolist())\n",
        "            total_labels.extend(labels.tolist())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % 10 == 9:  # Print average loss every 10 batches\n",
        "                average_loss = running_loss / 10\n",
        "                accuracy, precision, recall = compute_metrics(torch.tensor(total_predictions), torch.tensor(total_labels))\n",
        "                print(f\"Epoch: {epoch}, Batch: {i+1}, Loss: {average_loss}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}\")\n",
        "                running_loss = 0.0\n",
        "\n",
        "                # Reset lists for next batch\n",
        "                total_predictions = []\n",
        "                total_labels = []\n",
        "\n",
        "    return running_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWUhv7b-WJMk",
        "outputId": "bd044904-19f2-4022-b23f-4974982c3188"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 1:\n",
            "Epoch: 0, Batch: 10, Loss: 0.6946913361549377, Accuracy: 56.72, Precision: 0.43, Recall: 0.02\n",
            "Epoch: 0, Batch: 20, Loss: 0.7055293619632721, Accuracy: 50.72, Precision: 0.56, Recall: 0.56\n",
            "Epoch: 0, Batch: 30, Loss: 0.6984784841537476, Accuracy: 46.0, Precision: 0.46, Recall: 1.0\n",
            "Epoch: 0, Batch: 40, Loss: 0.6948852896690368, Accuracy: 50.24, Precision: 0.53, Recall: 0.54\n",
            "Epoch: 0, Batch: 50, Loss: 0.6954054117202759, Accuracy: 50.0, Precision: 0.5, Recall: 1.0\n",
            "Epoch: 0, Batch: 60, Loss: 0.6918202757835388, Accuracy: 50.72, Precision: 0.51, Recall: 0.86\n",
            "Epoch: 0, Batch: 70, Loss: 0.689546400308609, Accuracy: 51.32, Precision: 0.44, Recall: 0.39\n",
            "Epoch: 0, Batch: 80, Loss: 0.7030269086360932, Accuracy: 44.0, Precision: 0.0, Recall: 0.0\n",
            "Epoch: 0, Batch: 90, Loss: 0.6888383388519287, Accuracy: 54.96, Precision: 0.58, Recall: 0.81\n",
            "Epoch: 0, Batch: 100, Loss: 0.6952540636062622, Accuracy: 50.0, Precision: 0.5, Recall: 0.99\n",
            "Epoch: 0, Batch: 110, Loss: 0.6886343061923981, Accuracy: 49.46, Precision: 0.49, Recall: 0.77\n",
            "Epoch: 0, Batch: 120, Loss: 0.6930143296718597, Accuracy: 52.28, Precision: 0.47, Recall: 0.12\n",
            "Epoch: 0, Batch: 130, Loss: 0.701816326379776, Accuracy: 50.78, Precision: 0.47, Recall: 0.37\n",
            "Epoch: 0, Batch: 140, Loss: 0.6856880903244018, Accuracy: 50.48, Precision: 0.48, Recall: 0.38\n",
            "Epoch: 0, Batch: 150, Loss: 0.6930035829544068, Accuracy: 54.6, Precision: 0.45, Recall: 0.04\n",
            "Epoch: 0, Batch: 160, Loss: 0.6960878193378448, Accuracy: 50.6, Precision: 0.56, Recall: 0.55\n",
            "Epoch: 0, Batch: 170, Loss: 0.6992231130599975, Accuracy: 47.48, Precision: 0.47, Recall: 0.92\n",
            "Epoch: 0, Batch: 180, Loss: 0.689452338218689, Accuracy: 50.44, Precision: 0.48, Recall: 0.39\n",
            "Epoch: 0, Batch: 190, Loss: 0.6805796325206757, Accuracy: 52.46, Precision: 0.47, Recall: 0.09\n",
            "Epoch: 0, Batch: 200, Loss: 0.6691840648651123, Accuracy: 50.9, Precision: 0.45, Recall: 0.41\n",
            "Epoch: 0, Batch: 210, Loss: 0.7113709926605225, Accuracy: 49.6, Precision: 0.51, Recall: 0.3\n",
            "Epoch: 0, Batch: 220, Loss: 0.6752303004264831, Accuracy: 49.28, Precision: 0.52, Recall: 0.32\n",
            "Epoch: 0, Batch: 230, Loss: 0.6998096525669097, Accuracy: 51.88, Precision: 0.52, Recall: 0.97\n",
            "Epoch: 0, Batch: 240, Loss: 0.6620634615421295, Accuracy: 50.24, Precision: 0.52, Recall: 0.56\n",
            "Epoch: 0, Batch: 250, Loss: 0.662263035774231, Accuracy: 51.8, Precision: 0.56, Recall: 0.65\n",
            "Epoch: 0, Batch: 260, Loss: 0.6300522327423096, Accuracy: 50.2, Precision: 0.52, Recall: 0.55\n",
            "Epoch: 0, Batch: 270, Loss: 0.6950991779565812, Accuracy: 54.34, Precision: 0.57, Recall: 0.81\n",
            "Epoch: 0, Batch: 280, Loss: 0.6469045460224152, Accuracy: 48.0, Precision: 0.6, Recall: 0.4\n",
            "Epoch: 0, Batch: 290, Loss: 0.6628379642963409, Accuracy: 52.22, Precision: 0.53, Recall: 0.87\n",
            "Epoch: 0, Batch: 300, Loss: 0.6681250751018524, Accuracy: 51.68, Precision: 0.54, Recall: 0.71\n",
            "Epoch: 0, Batch: 310, Loss: 0.6593072831630706, Accuracy: 50.48, Precision: 0.46, Recall: 0.44\n",
            "Epoch: 0, Batch: 320, Loss: 0.6847145736217499, Accuracy: 47.92, Precision: 0.54, Recall: 0.24\n",
            "Epoch: 0, Batch: 330, Loss: 0.6719464480876922, Accuracy: 49.28, Precision: 0.48, Recall: 0.68\n",
            "Epoch: 0, Batch: 340, Loss: 0.6822583079338074, Accuracy: 51.36, Precision: 0.46, Recall: 0.33\n",
            "Epoch: 0, Batch: 350, Loss: 0.6743464946746827, Accuracy: 48.5, Precision: 0.55, Recall: 0.35\n",
            "Epoch: 0, Batch: 360, Loss: 0.680009788274765, Accuracy: 48.8, Precision: 0.47, Recall: 0.7\n",
            "Epoch: 0, Batch: 370, Loss: 0.6398224651813507, Accuracy: 49.86, Precision: 0.49, Recall: 0.57\n",
            "Epoch: 0, Batch: 380, Loss: 0.6074857711791992, Accuracy: 53.6, Precision: 0.4, Recall: 0.32\n",
            "Epoch: 0, Batch: 390, Loss: 0.7030974507331849, Accuracy: 50.06, Precision: 0.51, Recall: 0.53\n",
            "Epoch: 0, Batch: 400, Loss: 0.5903106331825256, Accuracy: 52.24, Precision: 0.64, Recall: 0.58\n",
            "Epoch: 0, Batch: 410, Loss: 0.5455142259597778, Accuracy: 53.52, Precision: 0.58, Recall: 0.72\n",
            "Epoch: 0, Batch: 420, Loss: 0.5867901712656021, Accuracy: 50.4, Precision: 0.52, Recall: 0.6\n",
            "Epoch: 0, Batch: 430, Loss: 0.6815642356872559, Accuracy: 50.5, Precision: 0.49, Recall: 0.25\n",
            "Epoch: 0, Batch: 440, Loss: 0.6256912171840667, Accuracy: 49.84, Precision: 0.46, Recall: 0.52\n",
            "Epoch: 0, Batch: 450, Loss: 0.6275319695472718, Accuracy: 50.44, Precision: 0.49, Recall: 0.28\n",
            "Epoch: 0, Batch: 460, Loss: 0.6426752269268036, Accuracy: 49.64, Precision: 0.51, Recall: 0.32\n",
            "Epoch: 0, Batch: 470, Loss: 0.6635232090950012, Accuracy: 50.96, Precision: 0.42, Recall: 0.44\n",
            "Epoch: 0, Batch: 480, Loss: 0.6096275269985199, Accuracy: 51.12, Precision: 0.48, Recall: 0.22\n",
            "Epoch: 0, Batch: 490, Loss: 0.5359040528535843, Accuracy: 52.72, Precision: 0.42, Recall: 0.33\n",
            "Epoch: 0, Batch: 500, Loss: 0.625962832570076, Accuracy: 50.8, Precision: 0.54, Recall: 0.6\n",
            "Epoch: 0, Batch: 510, Loss: 0.6497816652059555, Accuracy: 50.0, Precision: 0.5, Recall: 0.55\n",
            "Epoch: 0, Batch: 520, Loss: 0.5962461411952973, Accuracy: 50.16, Precision: 0.58, Recall: 0.51\n",
            "Epoch: 0, Batch: 530, Loss: 0.5972515940666199, Accuracy: 50.24, Precision: 0.51, Recall: 0.62\n",
            "Epoch: 0, Batch: 540, Loss: 0.5739110469818115, Accuracy: 50.0, Precision: 0.5, Recall: 0.49\n",
            "Epoch: 0, Batch: 550, Loss: 0.6707102835178376, Accuracy: 50.32, Precision: 0.48, Recall: 0.42\n",
            "Epoch: 0, Batch: 560, Loss: 0.6256089806556702, Accuracy: 50.84, Precision: 0.48, Recall: 0.29\n",
            "Epoch: 0, Batch: 570, Loss: 0.6325082182884216, Accuracy: 50.32, Precision: 0.51, Recall: 0.66\n",
            "Epoch: 0, Batch: 580, Loss: 0.6349196910858155, Accuracy: 51.4, Precision: 0.57, Recall: 0.6\n",
            "Epoch: 0, Batch: 590, Loss: 0.6532092332839966, Accuracy: 50.72, Precision: 0.44, Recall: 0.44\n",
            "Epoch: 0, Batch: 600, Loss: 0.711163318157196, Accuracy: 51.6, Precision: 0.46, Recall: 0.3\n",
            "Epoch: 0, Batch: 610, Loss: 0.6212069064378738, Accuracy: 50.0, Precision: 0.5, Recall: 0.42\n",
            "Epoch: 0, Batch: 620, Loss: 0.6181085407733917, Accuracy: 50.32, Precision: 0.46, Recall: 0.46\n",
            "Epoch: 0, Batch: 630, Loss: 0.6429745972156524, Accuracy: 50.8, Precision: 0.48, Recall: 0.3\n",
            "Epoch: 0, Batch: 640, Loss: 0.6490863412618637, Accuracy: 50.48, Precision: 0.46, Recall: 0.44\n",
            "Epoch: 0, Batch: 650, Loss: 0.6235732138156891, Accuracy: 51.36, Precision: 0.46, Recall: 0.33\n",
            "Epoch: 0, Batch: 660, Loss: 0.5881115913391113, Accuracy: 50.6, Precision: 0.44, Recall: 0.45\n",
            "Epoch: 0, Batch: 670, Loss: 0.5821356177330017, Accuracy: 52.04, Precision: 0.44, Recall: 0.33\n",
            "Epoch: 0, Batch: 680, Loss: 0.5960320293903351, Accuracy: 50.72, Precision: 0.41, Recall: 0.46\n",
            "Epoch: 0, Batch: 690, Loss: 0.6342764228582383, Accuracy: 49.58, Precision: 0.53, Recall: 0.43\n",
            "Epoch: 0, Batch: 700, Loss: 0.6700454711914062, Accuracy: 50.08, Precision: 0.51, Recall: 0.54\n",
            "Epoch: 0, Batch: 710, Loss: 0.5837796092033386, Accuracy: 50.28, Precision: 0.43, Recall: 0.48\n",
            "Epoch: 0, Batch: 720, Loss: 0.6423011720180511, Accuracy: 51.56, Precision: 0.47, Recall: 0.24\n",
            "Epoch: 0, Batch: 730, Loss: 0.6195807069540024, Accuracy: 53.04, Precision: 0.58, Recall: 0.69\n",
            "Epoch: 0, Batch: 740, Loss: 0.6418293714523315, Accuracy: 50.0, Precision: 0.5, Recall: 0.83\n",
            "Epoch: 0, Batch: 750, Loss: 0.6280053675174713, Accuracy: 50.4, Precision: 0.48, Recall: 0.4\n",
            "Epoch: 0, Batch: 760, Loss: 0.6029541552066803, Accuracy: 49.68, Precision: 0.52, Recall: 0.42\n",
            "Epoch: 0, Batch: 770, Loss: 0.6088797479867936, Accuracy: 50.48, Precision: 0.53, Recall: 0.58\n",
            "Epoch: 0, Batch: 780, Loss: 0.6180805623531341, Accuracy: 49.76, Precision: 0.49, Recall: 0.62\n",
            "Epoch: 0, Batch: 790, Loss: 0.6113592326641083, Accuracy: 49.82, Precision: 0.53, Recall: 0.47\n",
            "Epoch: 0, Batch: 800, Loss: 0.6119854927062989, Accuracy: 51.4, Precision: 0.57, Recall: 0.6\n",
            "Epoch: 0, Batch: 810, Loss: 0.6381923198699951, Accuracy: 49.52, Precision: 0.48, Recall: 0.62\n",
            "Epoch: 0, Batch: 820, Loss: 0.6503694295883179, Accuracy: 50.24, Precision: 0.49, Recall: 0.38\n",
            "Epoch: 0, Batch: 830, Loss: 0.6049708485603332, Accuracy: 50.56, Precision: 0.46, Recall: 0.43\n",
            "Epoch: 0, Batch: 840, Loss: 0.5925354927778244, Accuracy: 49.92, Precision: 0.51, Recall: 0.46\n",
            "Epoch: 0, Batch: 850, Loss: 0.5898626565933227, Accuracy: 50.12, Precision: 0.47, Recall: 0.48\n",
            "Epoch: 0, Batch: 860, Loss: 0.5788278728723526, Accuracy: 51.44, Precision: 0.58, Recall: 0.59\n",
            "Epoch: 0, Batch: 870, Loss: 0.6577244997024536, Accuracy: 50.42, Precision: 0.51, Recall: 0.71\n",
            "Epoch: 0, Batch: 880, Loss: 0.5703005850315094, Accuracy: 51.92, Precision: 0.42, Recall: 0.38\n",
            "Epoch: 0, Batch: 890, Loss: 0.7453591883182525, Accuracy: 47.8, Precision: 0.55, Recall: 0.28\n",
            "Epoch: 0, Batch: 900, Loss: 0.6112593382596969, Accuracy: 51.44, Precision: 0.53, Recall: 0.74\n",
            "Epoch: 0, Batch: 910, Loss: 0.6008993446826935, Accuracy: 54.14, Precision: 0.59, Recall: 0.73\n",
            "Epoch: 0, Batch: 920, Loss: 0.6365426808595658, Accuracy: 49.8, Precision: 0.45, Recall: 0.52\n",
            "Epoch: 0, Batch: 930, Loss: 0.628024035692215, Accuracy: 48.88, Precision: 0.57, Recall: 0.42\n",
            "Epoch: 0, Batch: 940, Loss: 0.6539003133773804, Accuracy: 48.86, Precision: 0.47, Recall: 0.69\n",
            "Epoch: 0, Batch: 950, Loss: 0.6387480407953262, Accuracy: 50.26, Precision: 0.49, Recall: 0.37\n",
            "Epoch: 0, Batch: 960, Loss: 0.6680488049983978, Accuracy: 50.7, Precision: 0.57, Recall: 0.55\n",
            "Epoch: 0, Batch: 970, Loss: 0.5778755068778991, Accuracy: 53.08, Precision: 0.57, Recall: 0.72\n",
            "Epoch: 0, Batch: 980, Loss: 0.5464486330747604, Accuracy: 51.4, Precision: 0.57, Recall: 0.6\n",
            "Epoch: 0, Batch: 990, Loss: 0.5652025133371353, Accuracy: 50.56, Precision: 0.46, Recall: 0.43\n",
            "Epoch: 0, Batch: 1000, Loss: 0.6126571536064148, Accuracy: 49.04, Precision: 0.44, Recall: 0.58\n",
            "Epoch: 0, Batch: 1010, Loss: 0.6004930824041367, Accuracy: 52.04, Precision: 0.44, Recall: 0.33\n",
            "Epoch: 0, Batch: 1020, Loss: 0.5509220957756042, Accuracy: 49.98, Precision: 0.51, Recall: 0.49\n",
            "Epoch: 0, Batch: 1030, Loss: 0.5596115440130234, Accuracy: 50.88, Precision: 0.54, Recall: 0.61\n",
            "Epoch: 0, Batch: 1040, Loss: 0.5407714277505875, Accuracy: 51.4, Precision: 0.57, Recall: 0.6\n",
            "Epoch: 0, Batch: 1050, Loss: 0.6027316391468048, Accuracy: 50.2, Precision: 0.45, Recall: 0.48\n",
            "Epoch: 0, Batch: 1060, Loss: 0.5492406725883484, Accuracy: 50.48, Precision: 0.48, Recall: 0.38\n",
            "Epoch: 0, Batch: 1070, Loss: 0.624757918715477, Accuracy: 50.04, Precision: 0.48, Recall: 0.49\n",
            "Epoch: 0, Batch: 1080, Loss: 0.6266074061393738, Accuracy: 50.04, Precision: 0.49, Recall: 0.48\n",
            "Epoch: 0, Batch: 1090, Loss: 0.5938404262065887, Accuracy: 49.58, Precision: 0.47, Recall: 0.57\n",
            "Epoch: 0, Batch: 1100, Loss: 0.617408549785614, Accuracy: 52.2, Precision: 0.45, Recall: 0.28\n",
            "Epoch: 0, Batch: 1110, Loss: 0.6294952362775803, Accuracy: 49.82, Precision: 0.51, Recall: 0.41\n",
            "Epoch: 0, Batch: 1120, Loss: 0.6280398428440094, Accuracy: 51.92, Precision: 0.56, Recall: 0.66\n",
            "Epoch: 0, Batch: 1130, Loss: 0.620333981513977, Accuracy: 50.08, Precision: 0.51, Recall: 0.54\n",
            "Epoch: 0, Batch: 1140, Loss: 0.5385932445526123, Accuracy: 54.2, Precision: 0.36, Recall: 0.35\n",
            "Epoch: 0, Batch: 1150, Loss: 0.6603577971458435, Accuracy: 49.56, Precision: 0.51, Recall: 0.28\n",
            "Epoch: 0, Batch: 1160, Loss: 0.6124346107244492, Accuracy: 53.78, Precision: 0.59, Recall: 0.71\n",
            "Epoch: 0, Batch: 1170, Loss: 0.573053389787674, Accuracy: 49.44, Precision: 0.46, Recall: 0.57\n",
            "Epoch: 0, Batch: 1180, Loss: 0.6140256583690643, Accuracy: 55.28, Precision: 0.39, Recall: 0.26\n",
            "Epoch: 0, Batch: 1190, Loss: 0.5770780175924302, Accuracy: 50.72, Precision: 0.48, Recall: 0.32\n",
            "Epoch: 0, Batch: 1200, Loss: 0.5942783802747726, Accuracy: 50.08, Precision: 0.52, Recall: 0.52\n",
            "Epoch: 0, Batch: 1210, Loss: 0.6479106664657592, Accuracy: 51.2, Precision: 0.56, Recall: 0.6\n",
            "Epoch: 0, Batch: 1220, Loss: 0.5754341542720794, Accuracy: 51.68, Precision: 0.44, Recall: 0.36\n",
            "Epoch: 0, Batch: 1230, Loss: 0.6020645081996918, Accuracy: 48.92, Precision: 0.56, Recall: 0.41\n",
            "Epoch: 0, Batch: 1240, Loss: 0.665503305196762, Accuracy: 49.28, Precision: 0.41, Recall: 0.54\n",
            "Epoch: 0, Batch: 1250, Loss: 0.5145149797201156, Accuracy: 58.1, Precision: 0.35, Recall: 0.23\n",
            "Epoch: 0, Batch: 1260, Loss: 0.6492346525192261, Accuracy: 47.9, Precision: 0.57, Recall: 0.35\n",
            "Epoch: 0, Batch: 1270, Loss: 0.5881875634193421, Accuracy: 49.28, Precision: 0.44, Recall: 0.56\n",
            "Epoch: 0, Batch: 1280, Loss: 0.5682199329137803, Accuracy: 50.12, Precision: 0.53, Recall: 0.52\n",
            "Epoch: 0, Batch: 1290, Loss: 0.5992263346910477, Accuracy: 51.44, Precision: 0.56, Recall: 0.62\n",
            "Epoch: 0, Batch: 1300, Loss: 0.5969455242156982, Accuracy: 50.6, Precision: 0.55, Recall: 0.56\n",
            "Epoch: 0, Batch: 1310, Loss: 0.52568898499012, Accuracy: 50.0, Precision: 0.5, Recall: 0.48\n",
            "Epoch: 0, Batch: 1320, Loss: 0.6056393444538116, Accuracy: 50.0, Precision: 0.48, Recall: 0.5\n",
            "Epoch: 0, Batch: 1330, Loss: 0.5491059094667434, Accuracy: 50.96, Precision: 0.46, Recall: 0.38\n",
            "Epoch: 0, Batch: 1340, Loss: 0.6233585447072982, Accuracy: 50.0, Precision: 0.5, Recall: 0.45\n",
            "Epoch: 0, Batch: 1350, Loss: 0.6818836063146592, Accuracy: 50.0, Precision: 0.41, Recall: 0.5\n",
            "Epoch: 0, Batch: 1360, Loss: 0.5571268618106842, Accuracy: 50.56, Precision: 0.48, Recall: 0.36\n",
            "Epoch: 0, Batch: 1370, Loss: 0.6172924131155014, Accuracy: 48.9, Precision: 0.55, Recall: 0.39\n",
            "Epoch: 0, Batch: 1380, Loss: 0.62167609333992, Accuracy: 50.0, Precision: 0.5, Recall: 0.55\n",
            "Epoch: 0, Batch: 1390, Loss: 0.5040459394454956, Accuracy: 50.0, Precision: 0.53, Recall: 0.5\n",
            "Epoch: 0, Batch: 1400, Loss: 0.5843007683753967, Accuracy: 50.06, Precision: 0.51, Recall: 0.53\n",
            "Epoch: 0, Batch: 1410, Loss: 0.6038979351520538, Accuracy: 51.92, Precision: 0.42, Recall: 0.38\n",
            "Epoch: 0, Batch: 1420, Loss: 0.604716819524765, Accuracy: 51.2, Precision: 0.44, Recall: 0.4\n",
            "Epoch: 0, Batch: 1430, Loss: 0.6422258973121643, Accuracy: 50.4, Precision: 0.49, Recall: 0.3\n",
            "Epoch: 0, Batch: 1440, Loss: 0.5479088634252548, Accuracy: 50.08, Precision: 0.46, Recall: 0.49\n",
            "Epoch: 0, Batch: 1450, Loss: 0.6112769424915314, Accuracy: 50.96, Precision: 0.47, Recall: 0.34\n",
            "Epoch: 0, Batch: 1460, Loss: 0.6052480638027191, Accuracy: 50.3, Precision: 0.55, Recall: 0.53\n",
            "Epoch: 0, Batch: 1470, Loss: 0.6429514110088348, Accuracy: 49.3, Precision: 0.43, Recall: 0.55\n",
            "Epoch: 0, Batch: 1480, Loss: 0.5951894909143448, Accuracy: 50.0, Precision: 0.5, Recall: 0.38\n",
            "Epoch: 0, Batch: 1490, Loss: 0.6024339854717254, Accuracy: 50.08, Precision: 0.48, Recall: 0.48\n",
            "Epoch: 0, Batch: 1500, Loss: 0.5491582006216049, Accuracy: 51.0, Precision: 0.6, Recall: 0.55\n",
            "Epoch: 0, Batch: 1510, Loss: 0.5292421758174897, Accuracy: 50.0, Precision: 0.5, Recall: 0.58\n",
            "Epoch: 0, Batch: 1520, Loss: 0.6266367465257645, Accuracy: 50.18, Precision: 0.49, Recall: 0.41\n",
            "Epoch: 0, Batch: 1530, Loss: 0.6667311608791351, Accuracy: 50.22, Precision: 0.49, Recall: 0.39\n",
            "Epoch: 0, Batch: 1540, Loss: 0.6138577729463577, Accuracy: 50.28, Precision: 0.51, Recall: 0.64\n",
            "Epoch: 0, Batch: 1550, Loss: 0.5459069788455964, Accuracy: 51.3, Precision: 0.45, Recall: 0.37\n",
            "Epoch: 0, Batch: 1560, Loss: 0.6068648844957352, Accuracy: 48.7, Precision: 0.55, Recall: 0.37\n",
            "Epoch: 0, Batch: 1570, Loss: 0.6288386046886444, Accuracy: 49.92, Precision: 0.49, Recall: 0.54\n",
            "Epoch: 0, Batch: 1580, Loss: 0.500722911953926, Accuracy: 53.24, Precision: 0.41, Recall: 0.32\n",
            "Epoch: 0, Batch: 1590, Loss: 0.5248175621032715, Accuracy: 50.24, Precision: 0.62, Recall: 0.51\n",
            "Epoch: 0, Batch: 1600, Loss: 0.6287676006555557, Accuracy: 50.0, Precision: 0.5, Recall: 0.55\n",
            "Epoch: 0, Batch: 1610, Loss: 0.6326064944267273, Accuracy: 51.28, Precision: 0.46, Recall: 0.34\n",
            "Epoch: 0, Batch: 1620, Loss: 0.5634716302156448, Accuracy: 50.0, Precision: 0.5, Recall: 0.51\n",
            "Epoch: 0, Batch: 1630, Loss: 0.5278279036283493, Accuracy: 52.4, Precision: 0.44, Recall: 0.3\n",
            "Epoch: 0, Batch: 1640, Loss: 0.565706530213356, Accuracy: 50.04, Precision: 0.51, Recall: 0.52\n",
            "Epoch: 0, Batch: 1650, Loss: 0.5630440920591354, Accuracy: 50.0, Precision: 0.58, Recall: 0.5\n",
            "Epoch: 0, Batch: 1660, Loss: 0.6119562566280365, Accuracy: 51.8, Precision: 0.55, Recall: 0.68\n",
            "Epoch: 0, Batch: 1670, Loss: 0.5478848546743393, Accuracy: 50.52, Precision: 0.48, Recall: 0.37\n",
            "Epoch: 0, Batch: 1680, Loss: 0.6580354750156403, Accuracy: 50.0, Precision: 0.5, Recall: 0.51\n",
            "Epoch: 0, Batch: 1690, Loss: 0.5996793001890183, Accuracy: 50.0, Precision: 0.58, Recall: 0.5\n",
            "Epoch: 0, Batch: 1700, Loss: 0.5768913090229034, Accuracy: 50.9, Precision: 0.55, Recall: 0.59\n",
            "Epoch: 0, Batch: 1710, Loss: 0.6342300623655319, Accuracy: 50.0, Precision: 0.5, Recall: 0.63\n",
            "Epoch: 0, Batch: 1720, Loss: 0.6007308453321457, Accuracy: 50.0, Precision: 0.5, Recall: 0.49\n",
            "Epoch: 0, Batch: 1730, Loss: 0.5573970049619674, Accuracy: 50.8, Precision: 0.46, Recall: 0.4\n",
            "Epoch: 0, Batch: 1740, Loss: 0.5419202834367752, Accuracy: 49.8, Precision: 0.48, Recall: 0.55\n",
            "Epoch: 0, Batch: 1750, Loss: 0.569058409333229, Accuracy: 49.84, Precision: 0.54, Recall: 0.48\n",
            "Epoch: 0, Batch: 1760, Loss: 0.5415351539850235, Accuracy: 49.7, Precision: 0.53, Recall: 0.45\n",
            "Epoch: 0, Batch: 1770, Loss: 0.60815709233284, Accuracy: 50.0, Precision: 0.5, Recall: 0.6\n",
            "Epoch: 0, Batch: 1780, Loss: 0.49105860888957975, Accuracy: 54.62, Precision: 0.39, Recall: 0.29\n",
            "Epoch: 0, Batch: 1790, Loss: 0.6462515950202942, Accuracy: 48.92, Precision: 0.59, Recall: 0.44\n",
            "Epoch: 0, Batch: 1800, Loss: 0.5440034538507461, Accuracy: 53.08, Precision: 0.57, Recall: 0.72\n",
            "Epoch: 0, Batch: 1810, Loss: 0.5630689859390259, Accuracy: 52.28, Precision: 0.56, Recall: 0.69\n",
            "Epoch: 0, Batch: 1820, Loss: 0.5380551189184188, Accuracy: 50.5, Precision: 0.45, Recall: 0.45\n",
            "Epoch: 0, Batch: 1830, Loss: 0.6064185380935669, Accuracy: 50.66, Precision: 0.47, Recall: 0.39\n",
            "Epoch: 0, Batch: 1840, Loss: 0.6462785750627518, Accuracy: 50.18, Precision: 0.59, Recall: 0.51\n",
            "Epoch: 0, Batch: 1850, Loss: 0.6270399987697601, Accuracy: 50.4, Precision: 0.54, Recall: 0.55\n",
            "Epoch: 0, Batch: 1860, Loss: 0.5479158878326416, Accuracy: 49.72, Precision: 0.49, Recall: 0.64\n",
            "Epoch: 0, Batch: 1870, Loss: 0.6068449914455414, Accuracy: 49.64, Precision: 0.52, Recall: 0.41\n",
            "Epoch: 0, Batch: 1880, Loss: 0.6090390294790268, Accuracy: 50.0, Precision: 0.5, Recall: 0.55\n",
            "Epoch: 0, Batch: 1890, Loss: 0.5309785515069961, Accuracy: 50.14, Precision: 0.49, Recall: 0.43\n",
            "Epoch: 0, Batch: 1900, Loss: 0.6193161636590958, Accuracy: 50.8, Precision: 0.54, Recall: 0.6\n",
            "Epoch: 0, Batch: 1910, Loss: 0.5231362283229828, Accuracy: 49.52, Precision: 0.52, Recall: 0.38\n",
            "Epoch: 0, Batch: 1920, Loss: 0.5686978727579117, Accuracy: 50.2, Precision: 0.52, Recall: 0.55\n",
            "Epoch: 0, Batch: 1930, Loss: 0.543807429075241, Accuracy: 50.24, Precision: 0.47, Recall: 0.46\n",
            "Epoch: 0, Batch: 1940, Loss: 0.598950007557869, Accuracy: 51.92, Precision: 0.42, Recall: 0.38\n",
            "Epoch: 0, Batch: 1950, Loss: 0.5586289644241333, Accuracy: 50.76, Precision: 0.48, Recall: 0.31\n",
            "Epoch: 0, Batch: 1960, Loss: 0.5554602742195129, Accuracy: 51.08, Precision: 0.44, Recall: 0.41\n",
            "Epoch: 0, Batch: 1970, Loss: 0.651480495929718, Accuracy: 48.4, Precision: 0.58, Recall: 0.4\n",
            "Epoch: 0, Batch: 1980, Loss: 0.6065668761730194, Accuracy: 50.32, Precision: 0.51, Recall: 0.66\n",
            "Epoch: 0, Batch: 1990, Loss: 0.6048808604478836, Accuracy: 53.08, Precision: 0.39, Recall: 0.36\n",
            "Epoch: 0, Batch: 2000, Loss: 0.580534291267395, Accuracy: 50.8, Precision: 0.48, Recall: 0.3\n",
            "Epoch: 0, Batch: 2010, Loss: 0.5317780017852783, Accuracy: 50.06, Precision: 0.53, Recall: 0.51\n",
            "Epoch: 0, Batch: 2020, Loss: 0.6426179647445679, Accuracy: 50.04, Precision: 0.51, Recall: 0.52\n",
            "Epoch: 0, Batch: 2030, Loss: 0.704168638586998, Accuracy: 50.0, Precision: 0.52, Recall: 0.5\n",
            "Epoch: 0, Batch: 2040, Loss: 0.5777274936437606, Accuracy: 49.6, Precision: 0.52, Recall: 0.4\n",
            "Epoch: 0, Batch: 2050, Loss: 0.5618992328643799, Accuracy: 49.98, Precision: 0.51, Recall: 0.49\n",
            "Epoch: 0, Batch: 2060, Loss: 0.6467100262641907, Accuracy: 48.56, Precision: 0.42, Recall: 0.59\n",
            "Epoch: 0, Batch: 2070, Loss: 0.5627646952867508, Accuracy: 50.0, Precision: 0.5, Recall: 0.34\n",
            "Epoch: 0, Batch: 2080, Loss: 0.5259541064500809, Accuracy: 51.92, Precision: 0.44, Recall: 0.34\n",
            "Epoch: 0, Batch: 2090, Loss: 0.5180441290140152, Accuracy: 50.48, Precision: 0.58, Recall: 0.53\n",
            "Epoch: 0, Batch: 2100, Loss: 0.5869460225105285, Accuracy: 50.56, Precision: 0.52, Recall: 0.64\n",
            "Epoch: 0, Batch: 2110, Loss: 0.5951895654201508, Accuracy: 50.56, Precision: 0.57, Recall: 0.54\n",
            "Epoch: 0, Batch: 2120, Loss: 0.6177287846803665, Accuracy: 50.18, Precision: 0.53, Recall: 0.53\n",
            "Epoch: 0, Batch: 2130, Loss: 0.55023013651371, Accuracy: 51.26, Precision: 0.57, Recall: 0.59\n",
            "Epoch: 0, Batch: 2140, Loss: 0.5998008728027344, Accuracy: 50.0, Precision: 0.45, Recall: 0.5\n",
            "Epoch: 0, Batch: 2150, Loss: 0.5158699840307236, Accuracy: 49.56, Precision: 0.52, Recall: 0.39\n",
            "Epoch: 0, Batch: 2160, Loss: 0.6071508765220642, Accuracy: 49.7, Precision: 0.47, Recall: 0.55\n",
            "Epoch: 0, Batch: 2170, Loss: 0.5756306320428848, Accuracy: 50.36, Precision: 0.56, Recall: 0.53\n",
            "Epoch: 0, Batch: 2180, Loss: 0.5626534253358841, Accuracy: 51.32, Precision: 0.56, Recall: 0.61\n",
            "Epoch: 0, Batch: 2190, Loss: 0.5626963436603546, Accuracy: 50.0, Precision: 0.51, Recall: 0.5\n",
            "Epoch: 0, Batch: 2200, Loss: 0.5705083221197128, Accuracy: 49.82, Precision: 0.47, Recall: 0.53\n",
            "Epoch: 0, Batch: 2210, Loss: 0.5918434768915176, Accuracy: 49.7, Precision: 0.51, Recall: 0.35\n",
            "Epoch: 0, Batch: 2220, Loss: 0.6206667184829712, Accuracy: 49.98, Precision: 0.49, Recall: 0.51\n",
            "Epoch: 0, Batch: 2230, Loss: 0.5992208510637284, Accuracy: 50.28, Precision: 0.52, Recall: 0.57\n",
            "Epoch: 0, Batch: 2240, Loss: 0.6082227468490601, Accuracy: 50.1, Precision: 0.49, Recall: 0.45\n",
            "Epoch: 0, Batch: 2250, Loss: 0.6317445039749146, Accuracy: 50.1, Precision: 0.51, Recall: 0.55\n",
            "Epoch: 0, Batch: 2260, Loss: 0.6307843863964081, Accuracy: 50.36, Precision: 0.48, Recall: 0.41\n",
            "Epoch: 0, Batch: 2270, Loss: 0.5580549508333206, Accuracy: 50.0, Precision: 0.5, Recall: 0.47\n",
            "Epoch: 0, Batch: 2280, Loss: 0.5659215360879898, Accuracy: 50.6, Precision: 0.53, Recall: 0.6\n",
            "Epoch: 0, Batch: 2290, Loss: 0.5454103946685791, Accuracy: 50.0, Precision: 0.5, Recall: 0.47\n",
            "Epoch: 0, Batch: 2300, Loss: 0.6251661449670791, Accuracy: 51.6, Precision: 0.58, Recall: 0.6\n",
            "Epoch: 0, Batch: 2310, Loss: 0.5793144762516022, Accuracy: 50.04, Precision: 0.52, Recall: 0.51\n",
            "Epoch: 0, Batch: 2320, Loss: 0.6080484211444854, Accuracy: 51.0, Precision: 0.4, Recall: 0.45\n",
            "Epoch: 0, Batch: 2330, Loss: 0.5815986186265946, Accuracy: 52.28, Precision: 0.44, Recall: 0.31\n",
            "Epoch: 0, Batch: 2340, Loss: 0.5830976486206054, Accuracy: 50.6, Precision: 0.56, Recall: 0.55\n",
            "Epoch: 0, Batch: 2350, Loss: 0.5757799506187439, Accuracy: 49.68, Precision: 0.46, Recall: 0.54\n",
            "Epoch: 0, Batch: 2360, Loss: 0.6055754572153091, Accuracy: 50.2, Precision: 0.49, Recall: 0.4\n",
            "Epoch: 0, Batch: 2370, Loss: 0.5546671360731125, Accuracy: 51.68, Precision: 0.44, Recall: 0.36\n",
            "Epoch: 0, Batch: 2380, Loss: 0.5747364670038223, Accuracy: 50.12, Precision: 0.49, Recall: 0.44\n",
            "Epoch: 0, Batch: 2390, Loss: 0.512092438340187, Accuracy: 49.94, Precision: 0.51, Recall: 0.47\n",
            "Epoch: 0, Batch: 2400, Loss: 0.501227679848671, Accuracy: 50.8, Precision: 0.55, Recall: 0.58\n",
            "Epoch: 0, Batch: 2410, Loss: 0.5851168602705001, Accuracy: 50.48, Precision: 0.47, Recall: 0.42\n",
            "Epoch: 0, Batch: 2420, Loss: 0.5832174897193909, Accuracy: 52.88, Precision: 0.44, Recall: 0.26\n",
            "Epoch: 0, Batch: 2430, Loss: 0.5885536462068558, Accuracy: 49.8, Precision: 0.52, Recall: 0.45\n",
            "Epoch: 0, Batch: 2440, Loss: 0.4837780684232712, Accuracy: 49.88, Precision: 0.47, Recall: 0.52\n",
            "Epoch: 0, Batch: 2450, Loss: 0.4576391041278839, Accuracy: 50.24, Precision: 0.47, Recall: 0.46\n",
            "Epoch: 0, Batch: 2460, Loss: 0.7042587906122207, Accuracy: 49.72, Precision: 0.52, Recall: 0.43\n",
            "Epoch: 0, Batch: 2470, Loss: 0.6437282085418701, Accuracy: 51.08, Precision: 0.59, Recall: 0.56\n",
            "Epoch: 0, Batch: 2480, Loss: 0.5805285900831223, Accuracy: 49.76, Precision: 0.53, Recall: 0.46\n",
            "Epoch: 0, Batch: 2490, Loss: 0.5615043818950654, Accuracy: 50.3, Precision: 0.55, Recall: 0.53\n",
            "Epoch: 0, Batch: 2500, Loss: 0.5448839783668518, Accuracy: 50.96, Precision: 0.56, Recall: 0.58\n",
            "Epoch: 0, Batch: 2510, Loss: 0.5677497118711472, Accuracy: 50.22, Precision: 0.51, Recall: 0.61\n",
            "Epoch: 0, Batch: 2520, Loss: 0.6541898429393769, Accuracy: 50.08, Precision: 0.49, Recall: 0.46\n",
            "Epoch: 0, Batch: 2530, Loss: 0.5747369825839996, Accuracy: 50.44, Precision: 0.48, Recall: 0.39\n",
            "Epoch: 0, Batch: 2540, Loss: 0.5604139477014541, Accuracy: 51.26, Precision: 0.43, Recall: 0.41\n",
            "Epoch: 0, Batch: 2550, Loss: 0.576391676068306, Accuracy: 50.22, Precision: 0.49, Recall: 0.39\n",
            "Epoch: 0, Batch: 2560, Loss: 0.5615426123142242, Accuracy: 50.56, Precision: 0.57, Recall: 0.54\n",
            "Epoch: 0, Batch: 2570, Loss: 0.555316099524498, Accuracy: 50.42, Precision: 0.43, Recall: 0.47\n",
            "Epoch: 0, Batch: 2580, Loss: 0.6454691857099533, Accuracy: 49.52, Precision: 0.58, Recall: 0.47\n",
            "Epoch: 0, Batch: 2590, Loss: 0.6496253460645676, Accuracy: 49.84, Precision: 0.48, Recall: 0.54\n",
            "Epoch: 0, Batch: 2600, Loss: 0.49777724146842955, Accuracy: 49.88, Precision: 0.53, Recall: 0.48\n",
            "Epoch: 0, Batch: 2610, Loss: 0.6056737065315246, Accuracy: 50.84, Precision: 0.57, Recall: 0.56\n",
            "Epoch: 0, Batch: 2620, Loss: 0.5819285422563553, Accuracy: 50.66, Precision: 0.53, Recall: 0.61\n",
            "Epoch: 0, Batch: 2630, Loss: 0.5641025573015213, Accuracy: 52.24, Precision: 0.42, Recall: 0.36\n",
            "Epoch: 0, Batch: 2640, Loss: 0.6292553693056107, Accuracy: 50.0, Precision: 0.5, Recall: 0.38\n",
            "Epoch: 0, Batch: 2650, Loss: 0.5323988646268845, Accuracy: 52.4, Precision: 0.4, Recall: 0.38\n",
            "Epoch: 0, Batch: 2660, Loss: 0.5428936153650283, Accuracy: 49.92, Precision: 0.52, Recall: 0.48\n",
            "Epoch: 0, Batch: 2670, Loss: 0.5039327800273895, Accuracy: 51.26, Precision: 0.43, Recall: 0.41\n",
            "Epoch: 0, Batch: 2680, Loss: 0.5680066853761673, Accuracy: 49.86, Precision: 0.57, Recall: 0.49\n",
            "Epoch: 0, Batch: 2690, Loss: 0.5869375199079514, Accuracy: 49.6, Precision: 0.46, Recall: 0.55\n",
            "Epoch: 0, Batch: 2700, Loss: 0.5891146421432495, Accuracy: 50.28, Precision: 0.49, Recall: 0.36\n",
            "Epoch: 0, Batch: 2710, Loss: 0.5233724474906921, Accuracy: 50.7, Precision: 0.55, Recall: 0.57\n",
            "Epoch: 0, Batch: 2720, Loss: 0.5909727126359939, Accuracy: 50.02, Precision: 0.51, Recall: 0.51\n",
            "Epoch: 0, Batch: 2730, Loss: 0.5912609368562698, Accuracy: 50.84, Precision: 0.57, Recall: 0.56\n",
            "Epoch: 0, Batch: 2740, Loss: 0.6079058051109314, Accuracy: 49.92, Precision: 0.46, Recall: 0.51\n",
            "Epoch: 0, Batch: 2750, Loss: 0.6075333386659623, Accuracy: 49.96, Precision: 0.51, Recall: 0.48\n",
            "Epoch: 0, Batch: 2760, Loss: 0.5948900401592254, Accuracy: 49.96, Precision: 0.51, Recall: 0.48\n",
            "Epoch: 0, Batch: 2770, Loss: 0.5953765958547592, Accuracy: 51.92, Precision: 0.58, Recall: 0.62\n",
            "Epoch: 0, Batch: 2780, Loss: 0.5842860788106918, Accuracy: 49.8, Precision: 0.49, Recall: 0.6\n",
            "Epoch: 0, Batch: 2790, Loss: 0.5469621062278748, Accuracy: 51.68, Precision: 0.43, Recall: 0.38\n",
            "Epoch: 0, Batch: 2800, Loss: 0.6180385202169418, Accuracy: 50.18, Precision: 0.47, Recall: 0.47\n",
            "Epoch: 0, Batch: 2810, Loss: 0.5821864128112793, Accuracy: 49.98, Precision: 0.51, Recall: 0.49\n",
            "Epoch: 0, Batch: 2820, Loss: 0.5578405171632767, Accuracy: 49.68, Precision: 0.48, Recall: 0.58\n",
            "Epoch: 0, Batch: 2830, Loss: 0.5237991660833359, Accuracy: 51.04, Precision: 0.46, Recall: 0.37\n",
            "Epoch: 0, Batch: 2840, Loss: 0.5250379979610443, Accuracy: 52.8, Precision: 0.43, Recall: 0.3\n",
            "Epoch: 0, Batch: 2850, Loss: 0.5804549634456635, Accuracy: 50.88, Precision: 0.46, Recall: 0.39\n",
            "Epoch: 0, Batch: 2860, Loss: 0.5665577560663223, Accuracy: 50.44, Precision: 0.52, Recall: 0.61\n",
            "Epoch: 0, Batch: 2870, Loss: 0.5815383493900299, Accuracy: 49.76, Precision: 0.56, Recall: 0.48\n",
            "Epoch: 0, Batch: 2880, Loss: 0.5751876324415207, Accuracy: 50.04, Precision: 0.49, Recall: 0.48\n",
            "Epoch: 0, Batch: 2890, Loss: 0.4957947075366974, Accuracy: 53.52, Precision: 0.39, Recall: 0.34\n",
            "Epoch: 0, Batch: 2900, Loss: 0.5321335345506668, Accuracy: 50.32, Precision: 0.48, Recall: 0.42\n",
            "Epoch: 0, Batch: 2910, Loss: 0.49120264053344725, Accuracy: 49.96, Precision: 0.52, Recall: 0.49\n",
            "Epoch: 0, Batch: 2920, Loss: 0.6284093737602234, Accuracy: 51.26, Precision: 0.41, Recall: 0.43\n",
            "Epoch: 0, Batch: 2930, Loss: 0.5547242075204849, Accuracy: 49.9, Precision: 0.51, Recall: 0.45\n",
            "Epoch: 0, Batch: 2940, Loss: 0.5452482491731644, Accuracy: 50.42, Precision: 0.53, Recall: 0.57\n",
            "Epoch: 0, Batch: 2950, Loss: 0.5979056835174561, Accuracy: 50.12, Precision: 0.56, Recall: 0.51\n",
            "Epoch: 0, Batch: 2960, Loss: 0.5379231184720993, Accuracy: 51.2, Precision: 0.6, Recall: 0.56\n",
            "Epoch: 0, Batch: 2970, Loss: 0.6307918071746826, Accuracy: 49.7, Precision: 0.47, Recall: 0.55\n",
            "Epoch: 0, Batch: 2980, Loss: 0.5608987033367157, Accuracy: 51.3, Precision: 0.63, Recall: 0.55\n",
            "Epoch: 0, Batch: 2990, Loss: 0.5769757270812989, Accuracy: 51.44, Precision: 0.41, Recall: 0.42\n",
            "Epoch: 0, Batch: 3000, Loss: 0.5815703809261322, Accuracy: 50.88, Precision: 0.46, Recall: 0.39\n",
            "Epoch: 0, Batch: 3010, Loss: 0.5876182675361633, Accuracy: 50.88, Precision: 0.46, Recall: 0.39\n",
            "Epoch: 0, Batch: 3020, Loss: 0.6172508627176285, Accuracy: 51.44, Precision: 0.44, Recall: 0.38\n",
            "Epoch: 0, Batch: 3030, Loss: 0.5107438385486602, Accuracy: 49.82, Precision: 0.51, Recall: 0.41\n",
            "Epoch: 0, Batch: 3040, Loss: 0.6072348982095719, Accuracy: 50.0, Precision: 0.48, Recall: 0.5\n",
            "Epoch: 0, Batch: 3050, Loss: 0.5832462310791016, Accuracy: 50.0, Precision: 0.5, Recall: 0.52\n",
            "Epoch: 0, Batch: 3060, Loss: 0.5557207077741623, Accuracy: 50.36, Precision: 0.48, Recall: 0.41\n",
            "Epoch: 0, Batch: 3070, Loss: 0.526951003074646, Accuracy: 50.48, Precision: 0.58, Recall: 0.53\n",
            "Epoch: 0, Batch: 3080, Loss: 0.5895793378353119, Accuracy: 49.84, Precision: 0.46, Recall: 0.52\n",
            "Epoch: 0, Batch: 3090, Loss: 0.5255197137594223, Accuracy: 49.96, Precision: 0.51, Recall: 0.48\n",
            "Epoch: 0, Batch: 3100, Loss: 0.5552977234125137, Accuracy: 51.44, Precision: 0.41, Recall: 0.42\n",
            "Epoch: 0, Batch: 3110, Loss: 0.6312141269445419, Accuracy: 49.72, Precision: 0.51, Recall: 0.36\n",
            "Epoch: 0, Batch: 3120, Loss: 0.5254264026880264, Accuracy: 51.1, Precision: 0.55, Recall: 0.61\n",
            "Epoch: 0, Batch: 3130, Loss: 0.5010383039712906, Accuracy: 50.0, Precision: 0.49, Recall: 0.5\n",
            "Epoch: 0, Batch: 3140, Loss: 0.5152183175086975, Accuracy: 50.42, Precision: 0.43, Recall: 0.47\n",
            "Epoch: 0, Batch: 3150, Loss: 0.5915025770664215, Accuracy: 50.52, Precision: 0.48, Recall: 0.37\n",
            "Epoch: 0, Batch: 3160, Loss: 0.5290502905845642, Accuracy: 50.84, Precision: 0.47, Recall: 0.36\n",
            "Epoch: 0, Batch: 3170, Loss: 0.5973759829998017, Accuracy: 50.0, Precision: 0.5, Recall: 0.58\n",
            "Epoch: 0, Batch: 3180, Loss: 0.558738899230957, Accuracy: 50.0, Precision: 0.5, Recall: 0.45\n",
            "Epoch: 0, Batch: 3190, Loss: 0.5523201256990433, Accuracy: 50.8, Precision: 0.45, Recall: 0.42\n",
            "Epoch: 0, Batch: 3200, Loss: 0.48900085389614106, Accuracy: 49.64, Precision: 0.53, Recall: 0.44\n",
            "Epoch: 0, Batch: 3210, Loss: 0.5621987700462341, Accuracy: 50.08, Precision: 0.52, Recall: 0.52\n",
            "Epoch: 0, Batch: 3220, Loss: 0.7080991357564926, Accuracy: 50.0, Precision: 0.48, Recall: 0.5\n",
            "Epoch: 0, Batch: 3230, Loss: 0.5887180119752884, Accuracy: 52.16, Precision: 0.59, Recall: 0.62\n",
            "Epoch: 0, Batch: 3240, Loss: 0.5367853671312333, Accuracy: 51.8, Precision: 0.56, Recall: 0.65\n",
            "Epoch: 0, Batch: 3250, Loss: 0.5259403824806214, Accuracy: 50.32, Precision: 0.52, Recall: 0.58\n",
            "Epoch: 0, Batch: 3260, Loss: 0.5716527909040451, Accuracy: 50.24, Precision: 0.54, Recall: 0.53\n",
            "Epoch: 0, Batch: 3270, Loss: 0.5749649554491043, Accuracy: 52.4, Precision: 0.58, Recall: 0.65\n",
            "Epoch: 0, Batch: 3280, Loss: 0.4543739676475525, Accuracy: 49.9, Precision: 0.55, Recall: 0.49\n",
            "Epoch: 0, Batch: 3290, Loss: 0.5618916422128677, Accuracy: 50.0, Precision: 0.5, Recall: 0.63\n",
            "Epoch: 0, Batch: 3300, Loss: 0.5459106177091598, Accuracy: 51.0, Precision: 0.45, Recall: 0.4\n",
            "Epoch: 0, Batch: 3310, Loss: 0.5954713851213456, Accuracy: 50.0, Precision: 0.5, Recall: 0.41\n",
            "Epoch: 0, Batch: 3320, Loss: 0.5947919547557831, Accuracy: 51.44, Precision: 0.56, Recall: 0.62\n",
            "Epoch: 0, Batch: 3330, Loss: 0.5469969779253006, Accuracy: 51.44, Precision: 0.56, Recall: 0.62\n",
            "Epoch: 0, Batch: 3340, Loss: 0.5735486954450607, Accuracy: 49.1, Precision: 0.55, Recall: 0.41\n",
            "Epoch: 0, Batch: 3350, Loss: 0.5528357177972794, Accuracy: 50.36, Precision: 0.44, Recall: 0.47\n",
            "Epoch: 0, Batch: 3360, Loss: 0.5037120282649994, Accuracy: 50.0, Precision: 0.49, Recall: 0.5\n",
            "Epoch: 0, Batch: 3370, Loss: 0.5421476900577545, Accuracy: 50.0, Precision: 0.59, Recall: 0.5\n",
            "Epoch: 0, Batch: 3380, Loss: 0.5747382402420044, Accuracy: 49.4, Precision: 0.4, Recall: 0.53\n",
            "Epoch: 0, Batch: 3390, Loss: 0.526851037144661, Accuracy: 49.16, Precision: 0.56, Recall: 0.43\n",
            "Epoch: 0, Batch: 3400, Loss: 0.4742168515920639, Accuracy: 50.12, Precision: 0.52, Recall: 0.53\n",
            "Epoch: 0, Batch: 3410, Loss: 0.4667985334992409, Accuracy: 50.54, Precision: 0.53, Recall: 0.59\n",
            "Epoch: 0, Batch: 3420, Loss: 0.5093649864196778, Accuracy: 50.84, Precision: 0.57, Recall: 0.56\n",
            "Epoch: 0, Batch: 3430, Loss: 0.5297369956970215, Accuracy: 49.94, Precision: 0.51, Recall: 0.47\n",
            "Epoch: 0, Batch: 3440, Loss: 0.5591126710176468, Accuracy: 49.52, Precision: 0.54, Recall: 0.44\n",
            "Epoch: 0, Batch: 3450, Loss: 0.5585698425769806, Accuracy: 50.12, Precision: 0.48, Recall: 0.47\n",
            "Epoch: 0, Batch: 3460, Loss: 0.49489682018756864, Accuracy: 49.68, Precision: 0.52, Recall: 0.42\n",
            "Epoch: 0, Batch: 3470, Loss: 0.6093624711036683, Accuracy: 50.32, Precision: 0.46, Recall: 0.46\n",
            "Epoch: 0, Batch: 3480, Loss: 0.5203620493412018, Accuracy: 49.12, Precision: 0.54, Recall: 0.39\n",
            "Epoch: 0, Batch: 3490, Loss: 0.5551304996013642, Accuracy: 49.44, Precision: 0.46, Recall: 0.57\n",
            "Epoch: 0, Batch: 3500, Loss: 0.5475442379713058, Accuracy: 49.28, Precision: 0.53, Recall: 0.38\n",
            "Epoch: 0, Batch: 3510, Loss: 0.5941703528165817, Accuracy: 50.0, Precision: 0.49, Recall: 0.5\n",
            "Epoch: 0, Batch: 3520, Loss: 0.47434998154640196, Accuracy: 50.28, Precision: 0.49, Recall: 0.36\n",
            "Epoch: 0, Batch: 3530, Loss: 0.5713719308376313, Accuracy: 50.36, Precision: 0.44, Recall: 0.47\n",
            "Epoch: 0, Batch: 3540, Loss: 0.5234348982572555, Accuracy: 51.04, Precision: 0.46, Recall: 0.37\n",
            "Epoch: 0, Batch: 3550, Loss: 0.4938106447458267, Accuracy: 50.7, Precision: 0.45, Recall: 0.43\n",
            "Epoch: 0, Batch: 3560, Loss: 0.5298018097877503, Accuracy: 49.76, Precision: 0.47, Recall: 0.54\n",
            "Epoch: 0, Batch: 3570, Loss: 0.5546513944864273, Accuracy: 49.8, Precision: 0.55, Recall: 0.48\n",
            "Epoch: 0, Batch: 3580, Loss: 0.6196118354797363, Accuracy: 50.9, Precision: 0.53, Recall: 0.65\n",
            "Epoch: 0, Batch: 3590, Loss: 0.5395897209644318, Accuracy: 54.48, Precision: 0.36, Recall: 0.34\n",
            "Epoch: 0, Batch: 3600, Loss: 0.5698231726884841, Accuracy: 50.0, Precision: 0.5, Recall: 0.29\n",
            "Epoch: 0, Batch: 3610, Loss: 0.5727950632572174, Accuracy: 50.64, Precision: 0.46, Recall: 0.42\n",
            "Epoch: 0, Batch: 3620, Loss: 0.481492692232132, Accuracy: 50.42, Precision: 0.43, Recall: 0.47\n",
            "Epoch: 0, Batch: 3630, Loss: 0.547178766131401, Accuracy: 49.16, Precision: 0.53, Recall: 0.36\n",
            "Epoch: 0, Batch: 3640, Loss: 0.5148642659187317, Accuracy: 50.64, Precision: 0.54, Recall: 0.58\n",
            "Epoch: 0, Batch: 3650, Loss: 0.578932809829712, Accuracy: 50.54, Precision: 0.53, Recall: 0.59\n",
            "Epoch: 0, Batch: 3660, Loss: 0.5317722588777543, Accuracy: 49.7, Precision: 0.55, Recall: 0.47\n",
            "Epoch: 0, Batch: 3670, Loss: 0.5255282253026963, Accuracy: 50.06, Precision: 0.51, Recall: 0.53\n",
            "Epoch: 0, Batch: 3680, Loss: 0.532829663157463, Accuracy: 49.88, Precision: 0.47, Recall: 0.52\n",
            "Epoch: 0, Batch: 3690, Loss: 0.5772238343954086, Accuracy: 50.28, Precision: 0.48, Recall: 0.43\n",
            "Epoch: 0, Batch: 3700, Loss: 0.5640265673398972, Accuracy: 49.96, Precision: 0.52, Recall: 0.49\n",
            "Epoch: 0, Batch: 3710, Loss: 0.5393802583217621, Accuracy: 51.92, Precision: 0.58, Recall: 0.62\n",
            "Epoch: 0, Batch: 3720, Loss: 0.572501614689827, Accuracy: 49.4, Precision: 0.44, Recall: 0.55\n",
            "Epoch: 0, Batch: 3730, Loss: 0.6045097798109055, Accuracy: 52.1, Precision: 0.43, Recall: 0.35\n",
            "Epoch: 0, Batch: 3740, Loss: 0.5951314598321915, Accuracy: 51.8, Precision: 0.45, Recall: 0.32\n",
            "Epoch: 0, Batch: 3750, Loss: 0.5277397930622101, Accuracy: 50.06, Precision: 0.53, Recall: 0.51\n",
            "Epoch: 0, Batch: 3760, Loss: 0.5306030422449112, Accuracy: 50.48, Precision: 0.58, Recall: 0.53\n",
            "Epoch: 0, Batch: 3770, Loss: 0.44881383031606675, Accuracy: 50.4, Precision: 0.45, Recall: 0.46\n",
            "Epoch: 0, Batch: 3780, Loss: 0.4681649476289749, Accuracy: 51.12, Precision: 0.43, Recall: 0.42\n",
            "Epoch: 0, Batch: 3790, Loss: 0.5944450467824935, Accuracy: 51.54, Precision: 0.61, Recall: 0.57\n",
            "Epoch: 0, Batch: 3800, Loss: 0.5367819875478744, Accuracy: 53.2, Precision: 0.6, Recall: 0.66\n",
            "Epoch: 0, Batch: 3810, Loss: 0.5817105919122696, Accuracy: 51.1, Precision: 0.39, Recall: 0.45\n",
            "Epoch: 0, Batch: 3820, Loss: 0.5105981856584549, Accuracy: 51.7, Precision: 0.45, Recall: 0.33\n",
            "Epoch: 0, Batch: 3830, Loss: 0.523120042681694, Accuracy: 50.64, Precision: 0.46, Recall: 0.42\n",
            "Epoch: 0, Batch: 3840, Loss: 0.4849455624818802, Accuracy: 49.72, Precision: 0.48, Recall: 0.57\n",
            "Epoch: 0, Batch: 3850, Loss: 0.5623989492654801, Accuracy: 51.4, Precision: 0.6, Recall: 0.57\n",
            "Epoch: 0, Batch: 3860, Loss: 0.46528003811836244, Accuracy: 50.0, Precision: 0.42, Recall: 0.5\n",
            "Epoch: 0, Batch: 3870, Loss: 0.5342388331890107, Accuracy: 50.4, Precision: 0.48, Recall: 0.4\n",
            "Epoch: 0, Batch: 3880, Loss: 0.5440406560897827, Accuracy: 51.04, Precision: 0.46, Recall: 0.37\n",
            "Epoch: 0, Batch: 3890, Loss: 0.6044445157051086, Accuracy: 50.08, Precision: 0.52, Recall: 0.52\n",
            "Epoch: 0, Batch: 3900, Loss: 0.5297596007585526, Accuracy: 50.36, Precision: 0.53, Recall: 0.56\n",
            "Epoch: 0, Batch: 3910, Loss: 0.5599589586257935, Accuracy: 50.84, Precision: 0.56, Recall: 0.57\n",
            "Epoch: 0, Batch: 3920, Loss: 0.5175485610961914, Accuracy: 50.2, Precision: 0.52, Recall: 0.55\n",
            "Epoch: 0, Batch: 3930, Loss: 0.437768492102623, Accuracy: 50.42, Precision: 0.53, Recall: 0.57\n",
            "Epoch: 0, Batch: 3940, Loss: 0.582069730758667, Accuracy: 50.42, Precision: 0.47, Recall: 0.43\n",
            "Epoch: 0, Batch: 3950, Loss: 0.5161273509263993, Accuracy: 49.9, Precision: 0.55, Recall: 0.49\n",
            "Epoch: 0, Batch: 3960, Loss: 0.6686240375041962, Accuracy: 49.64, Precision: 0.44, Recall: 0.53\n",
            "Epoch: 0, Batch: 3970, Loss: 0.5810535997152328, Accuracy: 49.52, Precision: 0.52, Recall: 0.38\n",
            "Epoch: 0, Batch: 3980, Loss: 0.6374577879905701, Accuracy: 50.02, Precision: 0.49, Recall: 0.49\n",
            "Epoch: 0, Batch: 3990, Loss: 0.5787728875875473, Accuracy: 51.12, Precision: 0.43, Recall: 0.42\n",
            "Epoch: 0, Batch: 4000, Loss: 0.5270177990198135, Accuracy: 49.78, Precision: 0.51, Recall: 0.39\n",
            "Epoch: 0, Batch: 4010, Loss: 0.5688518315553666, Accuracy: 50.0, Precision: 0.45, Recall: 0.5\n",
            "Epoch: 0, Batch: 4020, Loss: 0.5418326288461686, Accuracy: 50.72, Precision: 0.44, Recall: 0.44\n",
            "Epoch: 0, Batch: 4030, Loss: 0.5842469751834869, Accuracy: 50.96, Precision: 0.46, Recall: 0.38\n",
            "Epoch: 0, Batch: 4040, Loss: 0.47927523851394654, Accuracy: 50.32, Precision: 0.46, Recall: 0.46\n",
            "Epoch: 0, Batch: 4050, Loss: 0.5843782603740693, Accuracy: 50.1, Precision: 0.55, Recall: 0.51\n",
            "Epoch: 0, Batch: 4060, Loss: 0.5007149845361709, Accuracy: 51.0, Precision: 0.55, Recall: 0.6\n",
            "Epoch: 0, Batch: 4070, Loss: 0.6052206367254257, Accuracy: 50.3, Precision: 0.47, Recall: 0.45\n",
            "Epoch: 0, Batch: 4080, Loss: 0.47527762949466706, Accuracy: 49.94, Precision: 0.53, Recall: 0.49\n",
            "Epoch: 0, Batch: 4090, Loss: 0.5401096820831299, Accuracy: 49.68, Precision: 0.46, Recall: 0.54\n",
            "Epoch: 0, Batch: 4100, Loss: 0.5831057578325272, Accuracy: 50.56, Precision: 0.54, Recall: 0.57\n",
            "Epoch: 0, Batch: 4110, Loss: 0.566472265124321, Accuracy: 49.92, Precision: 0.52, Recall: 0.48\n",
            "Epoch: 0, Batch: 4120, Loss: 0.5138108968734741, Accuracy: 51.3, Precision: 0.45, Recall: 0.37\n",
            "Epoch: 0, Batch: 4130, Loss: 0.46633497178554534, Accuracy: 49.7, Precision: 0.55, Recall: 0.47\n",
            "Epoch: 0, Batch: 4140, Loss: 0.4831583648920059, Accuracy: 50.48, Precision: 0.53, Recall: 0.58\n",
            "Epoch: 0, Batch: 4150, Loss: 0.4850131869316101, Accuracy: 53.06, Precision: 0.41, Recall: 0.33\n",
            "Epoch: 0, Batch: 4160, Loss: 0.5782512336969375, Accuracy: 49.76, Precision: 0.46, Recall: 0.53\n",
            "Epoch: 0, Batch: 4170, Loss: 0.5308283627033233, Accuracy: 50.26, Precision: 0.49, Recall: 0.37\n",
            "Epoch: 0, Batch: 4180, Loss: 0.5235977709293366, Accuracy: 50.16, Precision: 0.48, Recall: 0.46\n",
            "Epoch: 0, Batch: 4190, Loss: 0.578376516699791, Accuracy: 50.66, Precision: 0.53, Recall: 0.61\n",
            "Epoch: 0, Batch: 4200, Loss: 0.598561578989029, Accuracy: 50.84, Precision: 0.43, Recall: 0.44\n",
            "Epoch: 0, Batch: 4210, Loss: 0.5430455058813095, Accuracy: 50.0, Precision: 0.5, Recall: 0.44\n",
            "Epoch: 0, Batch: 4220, Loss: 0.47171561419963837, Accuracy: 50.08, Precision: 0.49, Recall: 0.46\n",
            "Epoch: 0, Batch: 4230, Loss: 0.4588511258363724, Accuracy: 50.28, Precision: 0.48, Recall: 0.43\n",
            "Epoch: 0, Batch: 4240, Loss: 0.5906224310398102, Accuracy: 51.28, Precision: 0.42, Recall: 0.42\n",
            "Epoch: 0, Batch: 4250, Loss: 0.5361937761306763, Accuracy: 49.86, Precision: 0.57, Recall: 0.49\n",
            "Epoch: 0, Batch: 4260, Loss: 0.5592517733573914, Accuracy: 50.28, Precision: 0.52, Recall: 0.57\n",
            "Epoch: 0, Batch: 4270, Loss: 0.5016463190317154, Accuracy: 50.0, Precision: 0.48, Recall: 0.5\n",
            "Epoch: 0, Batch: 4280, Loss: 0.5856650084257126, Accuracy: 51.44, Precision: 0.41, Recall: 0.42\n",
            "Epoch: 0, Batch: 4290, Loss: 0.6251122146844864, Accuracy: 48.56, Precision: 0.56, Recall: 0.38\n",
            "Epoch: 0, Batch: 4300, Loss: 0.5075122773647308, Accuracy: 49.46, Precision: 0.47, Recall: 0.59\n",
            "Epoch: 0, Batch: 4310, Loss: 0.4990306466817856, Accuracy: 51.44, Precision: 0.59, Recall: 0.58\n",
            "Epoch: 0, Batch: 4320, Loss: 0.5310111463069915, Accuracy: 50.08, Precision: 0.51, Recall: 0.54\n",
            "Epoch: 0, Batch: 4330, Loss: 0.5718220710754395, Accuracy: 50.08, Precision: 0.52, Recall: 0.52\n",
            "Epoch: 0, Batch: 4340, Loss: 0.48503170907497406, Accuracy: 50.4, Precision: 0.45, Recall: 0.46\n",
            "Epoch: 0, Batch: 4350, Loss: 0.5052884876728058, Accuracy: 49.92, Precision: 0.52, Recall: 0.48\n",
            "Epoch: 0, Batch: 4360, Loss: 0.5094756305217742, Accuracy: 50.64, Precision: 0.42, Recall: 0.46\n",
            "Epoch: 0, Batch: 4370, Loss: 0.520146644115448, Accuracy: 49.7, Precision: 0.53, Recall: 0.45\n",
            "Epoch: 0, Batch: 4380, Loss: 0.5753240615129471, Accuracy: 51.68, Precision: 0.43, Recall: 0.38\n",
            "Epoch: 0, Batch: 4390, Loss: 0.5532364904880523, Accuracy: 49.72, Precision: 0.57, Recall: 0.48\n",
            "Epoch: 0, Batch: 4400, Loss: 0.4650274932384491, Accuracy: 50.24, Precision: 0.47, Recall: 0.46\n",
            "Epoch: 0, Batch: 4410, Loss: 0.5696821063756943, Accuracy: 53.52, Precision: 0.39, Recall: 0.34\n",
            "Epoch: 0, Batch: 4420, Loss: 0.6211403876543045, Accuracy: 51.2, Precision: 0.45, Recall: 0.38\n",
            "Epoch: 0, Batch: 4430, Loss: 0.47009126245975497, Accuracy: 51.92, Precision: 0.44, Recall: 0.34\n",
            "Epoch: 0, Batch: 4440, Loss: 0.4961139798164368, Accuracy: 49.88, Precision: 0.51, Recall: 0.44\n",
            "Epoch: 0, Batch: 4450, Loss: 0.6338052123785018, Accuracy: 51.08, Precision: 0.53, Recall: 0.68\n",
            "Epoch: 0, Batch: 4460, Loss: 0.5058708935976028, Accuracy: 52.8, Precision: 0.43, Recall: 0.3\n",
            "Epoch: 0, Batch: 4470, Loss: 0.5373165905475616, Accuracy: 49.22, Precision: 0.53, Recall: 0.37\n",
            "Epoch: 0, Batch: 4480, Loss: 0.5223189800977707, Accuracy: 49.92, Precision: 0.49, Recall: 0.54\n",
            "Epoch: 0, Batch: 4490, Loss: 0.5426018506288528, Accuracy: 50.04, Precision: 0.49, Recall: 0.48\n",
            "Epoch: 0, Batch: 4500, Loss: 0.510238516330719, Accuracy: 50.0, Precision: 0.5, Recall: 0.5\n",
            "Epoch: 0, Batch: 4510, Loss: 0.5735248297452926, Accuracy: 50.02, Precision: 0.51, Recall: 0.51\n",
            "Epoch: 0, Batch: 4520, Loss: 0.45448454320430753, Accuracy: 50.72, Precision: 0.44, Recall: 0.44\n",
            "Epoch: 0, Batch: 4530, Loss: 0.5580585926771164, Accuracy: 50.14, Precision: 0.49, Recall: 0.43\n",
            "Epoch: 0, Batch: 4540, Loss: 0.496482789516449, Accuracy: 50.0, Precision: 0.52, Recall: 0.5\n",
            "Epoch: 0, Batch: 4550, Loss: 0.5427178740501404, Accuracy: 50.08, Precision: 0.52, Recall: 0.52\n",
            "Epoch: 0, Batch: 4560, Loss: 0.49091637432575225, Accuracy: 50.24, Precision: 0.53, Recall: 0.54\n",
            "Epoch: 0, Batch: 4570, Loss: 0.5085261404514313, Accuracy: 51.1, Precision: 0.55, Recall: 0.61\n",
            "Epoch: 0, Batch: 4580, Loss: 0.5030544131994248, Accuracy: 49.82, Precision: 0.49, Recall: 0.59\n",
            "Epoch: 0, Batch: 4590, Loss: 0.6103546887636184, Accuracy: 52.88, Precision: 0.42, Recall: 0.32\n",
            "Epoch: 0, Batch: 4600, Loss: 0.5810620754957199, Accuracy: 50.12, Precision: 0.47, Recall: 0.48\n",
            "Epoch: 0, Batch: 4610, Loss: 0.4850876837968826, Accuracy: 49.88, Precision: 0.48, Recall: 0.53\n",
            "Epoch: 0, Batch: 4620, Loss: 0.5462451159954071, Accuracy: 50.16, Precision: 0.48, Recall: 0.46\n",
            "Epoch: 0, Batch: 4630, Loss: 0.5381410658359528, Accuracy: 50.7, Precision: 0.43, Recall: 0.45\n",
            "Epoch: 0, Batch: 4640, Loss: 0.4811903566122055, Accuracy: 50.56, Precision: 0.46, Recall: 0.43\n",
            "Epoch: 0, Batch: 4650, Loss: 0.6516521394252777, Accuracy: 50.44, Precision: 0.48, Recall: 0.39\n",
            "Epoch: 0, Batch: 4660, Loss: 0.5239863425493241, Accuracy: 53.6, Precision: 0.6, Recall: 0.68\n",
            "Epoch: 0, Batch: 4670, Loss: 0.6087546646595001, Accuracy: 49.96, Precision: 0.48, Recall: 0.51\n",
            "Epoch: 0, Batch: 4680, Loss: 0.5317140579223633, Accuracy: 51.82, Precision: 0.43, Recall: 0.37\n",
            "Epoch: 0, Batch: 4690, Loss: 0.5332060635089875, Accuracy: 53.2, Precision: 0.4, Recall: 0.34\n",
            "Epoch: 0, Batch: 4700, Loss: 0.5418554812669754, Accuracy: 49.74, Precision: 0.51, Recall: 0.37\n",
            "Epoch: 0, Batch: 4710, Loss: 0.5376019030809402, Accuracy: 49.56, Precision: 0.48, Recall: 0.61\n",
            "Epoch: 0, Batch: 4720, Loss: 0.5391945868730545, Accuracy: 50.42, Precision: 0.47, Recall: 0.43\n",
            "Epoch: 0, Batch: 4730, Loss: 0.5037838608026505, Accuracy: 49.92, Precision: 0.54, Recall: 0.49\n",
            "Epoch: 0, Batch: 4740, Loss: 0.4959391921758652, Accuracy: 51.12, Precision: 0.42, Recall: 0.43\n",
            "Epoch: 0, Batch: 4750, Loss: 0.5782240033149719, Accuracy: 50.28, Precision: 0.48, Recall: 0.43\n",
            "Epoch: 0, Batch: 4760, Loss: 0.5591335505247116, Accuracy: 50.6, Precision: 0.55, Recall: 0.56\n",
            "Epoch: 0, Batch: 4770, Loss: 0.5897104173898697, Accuracy: 49.64, Precision: 0.56, Recall: 0.47\n",
            "Epoch: 0, Batch: 4780, Loss: 0.5607311397790908, Accuracy: 53.22, Precision: 0.57, Recall: 0.73\n",
            "Epoch: 0, Batch: 4790, Loss: 0.5209327518939972, Accuracy: 49.94, Precision: 0.49, Recall: 0.53\n",
            "Epoch: 0, Batch: 4800, Loss: 0.5252994567155838, Accuracy: 49.76, Precision: 0.54, Recall: 0.47\n",
            "Epoch: 0, Batch: 4810, Loss: 0.5541159719228744, Accuracy: 49.58, Precision: 0.57, Recall: 0.47\n",
            "Epoch: 0, Batch: 4820, Loss: 0.5721464306116104, Accuracy: 49.58, Precision: 0.47, Recall: 0.57\n",
            "Epoch: 0, Batch: 4830, Loss: 0.5593389630317688, Accuracy: 51.44, Precision: 0.59, Recall: 0.58\n",
            "Epoch: 0, Batch: 4840, Loss: 0.5520229697227478, Accuracy: 50.56, Precision: 0.52, Recall: 0.64\n",
            "Epoch: 0, Batch: 4850, Loss: 0.5452042818069458, Accuracy: 52.52, Precision: 0.43, Recall: 0.32\n",
            "Epoch: 0, Batch: 4860, Loss: 0.48992128372192384, Accuracy: 50.0, Precision: 0.5, Recall: 0.42\n",
            "Epoch: 0, Batch: 4870, Loss: 0.577543492615223, Accuracy: 49.92, Precision: 0.46, Recall: 0.51\n",
            "Epoch: 0, Batch: 4880, Loss: 0.5400499999523163, Accuracy: 53.12, Precision: 0.38, Recall: 0.37\n",
            "Epoch: 0, Batch: 4890, Loss: 0.48992624282836916, Accuracy: 49.92, Precision: 0.51, Recall: 0.46\n",
            "Epoch: 0, Batch: 4900, Loss: 0.4773755013942719, Accuracy: 50.8, Precision: 0.55, Recall: 0.58\n",
            "Epoch: 0, Batch: 4910, Loss: 0.5550328880548477, Accuracy: 50.0, Precision: 0.52, Recall: 0.5\n",
            "Epoch: 0, Batch: 4920, Loss: 0.5951222747564315, Accuracy: 49.68, Precision: 0.48, Recall: 0.58\n",
            "Epoch: 0, Batch: 4930, Loss: 0.60189348757267, Accuracy: 48.08, Precision: 0.58, Recall: 0.38\n",
            "Epoch: 0, Batch: 4940, Loss: 0.5449694007635116, Accuracy: 48.46, Precision: 0.39, Recall: 0.57\n",
            "Epoch: 0, Batch: 4950, Loss: 0.4933509469032288, Accuracy: 49.92, Precision: 0.48, Recall: 0.52\n",
            "Epoch: 0, Batch: 4960, Loss: 0.5294979751110077, Accuracy: 52.04, Precision: 0.44, Recall: 0.33\n",
            "Epoch: 0, Batch: 4970, Loss: 0.5547226369380951, Accuracy: 50.18, Precision: 0.47, Recall: 0.47\n",
            "Epoch: 0, Batch: 4980, Loss: 0.59902703166008, Accuracy: 50.0, Precision: 0.51, Recall: 0.5\n",
            "Epoch: 0, Batch: 4990, Loss: 0.5533198922872543, Accuracy: 50.08, Precision: 0.49, Recall: 0.46\n",
            "Epoch: 0, Batch: 5000, Loss: 0.5280234545469285, Accuracy: 51.2, Precision: 0.44, Recall: 0.4\n",
            "Epoch: 0, Batch: 5010, Loss: 0.4880580246448517, Accuracy: 52.38, Precision: 0.43, Recall: 0.33\n",
            "Epoch: 0, Batch: 5020, Loss: 0.5913946747779846, Accuracy: 49.76, Precision: 0.53, Recall: 0.46\n",
            "Epoch: 0, Batch: 5030, Loss: 0.5491606652736664, Accuracy: 49.76, Precision: 0.47, Recall: 0.54\n",
            "Epoch: 0, Batch: 5040, Loss: 0.4691788345575333, Accuracy: 49.94, Precision: 0.51, Recall: 0.47\n",
            "Epoch: 0, Batch: 5050, Loss: 0.47190977931022643, Accuracy: 52.52, Precision: 0.41, Recall: 0.36\n",
            "Epoch: 0, Batch: 5060, Loss: 0.5395241931080819, Accuracy: 49.98, Precision: 0.51, Recall: 0.49\n",
            "Epoch: 0, Batch: 5070, Loss: 0.6857165157794952, Accuracy: 50.7, Precision: 0.57, Recall: 0.55\n",
            "Epoch: 0, Batch: 5080, Loss: 0.5144271582365036, Accuracy: 52.04, Precision: 0.56, Recall: 0.67\n",
            "Epoch: 0, Batch: 5090, Loss: 0.5092555046081543, Accuracy: 49.96, Precision: 0.49, Recall: 0.52\n",
            "Epoch: 0, Batch: 5100, Loss: 0.5542065769433975, Accuracy: 50.56, Precision: 0.46, Recall: 0.43\n",
            "Epoch: 0, Batch: 5110, Loss: 0.6603914946317673, Accuracy: 50.96, Precision: 0.47, Recall: 0.34\n",
            "Epoch: 0, Batch: 5120, Loss: 0.4700961112976074, Accuracy: 50.0, Precision: 0.54, Recall: 0.5\n",
            "Epoch: 0, Batch: 5130, Loss: 0.5045523315668106, Accuracy: 53.4, Precision: 0.6, Recall: 0.67\n",
            "Epoch: 0, Batch: 5140, Loss: 0.4845654100179672, Accuracy: 50.32, Precision: 0.46, Recall: 0.46\n",
            "Epoch: 0, Batch: 5150, Loss: 0.5020319938659668, Accuracy: 50.22, Precision: 0.49, Recall: 0.39\n",
            "Epoch: 0, Batch: 5160, Loss: 0.5212714970111847, Accuracy: 50.0, Precision: 0.51, Recall: 0.5\n",
            "Epoch: 0, Batch: 5170, Loss: 0.5148167163133621, Accuracy: 49.88, Precision: 0.48, Recall: 0.53\n",
            "Epoch: 0, Batch: 5180, Loss: 0.5100481808185577, Accuracy: 50.24, Precision: 0.49, Recall: 0.38\n",
            "Epoch: 0, Batch: 5190, Loss: 0.4163615971803665, Accuracy: 50.64, Precision: 0.46, Recall: 0.42\n",
            "Epoch: 0, Batch: 5200, Loss: 0.5389633119106293, Accuracy: 50.18, Precision: 0.47, Recall: 0.47\n",
            "Epoch: 0, Batch: 5210, Loss: 0.4248410761356354, Accuracy: 51.2, Precision: 0.46, Recall: 0.35\n",
            "Epoch: 0, Batch: 5220, Loss: 0.6274090051651001, Accuracy: 51.6, Precision: 0.55, Recall: 0.66\n",
            "Epoch: 0, Batch: 5230, Loss: 0.5041431307792663, Accuracy: 51.2, Precision: 0.4, Recall: 0.44\n",
            "Epoch: 0, Batch: 5240, Loss: 0.6197155386209487, Accuracy: 49.36, Precision: 0.58, Recall: 0.46\n",
            "Epoch: 0, Batch: 5250, Loss: 0.569561442732811, Accuracy: 50.32, Precision: 0.52, Recall: 0.58\n",
            "Epoch: 0, Batch: 5260, Loss: 0.5121960073709488, Accuracy: 50.6, Precision: 0.53, Recall: 0.6\n",
            "Epoch: 0, Batch: 5270, Loss: 0.48286661952733995, Accuracy: 51.0, Precision: 0.45, Recall: 0.4\n",
            "Epoch: 0, Batch: 5280, Loss: 0.5556558132171631, Accuracy: 50.0, Precision: 0.55, Recall: 0.5\n",
            "Epoch: 0, Batch: 5290, Loss: 0.51837617456913, Accuracy: 51.3, Precision: 0.55, Recall: 0.63\n",
            "Epoch: 0, Batch: 5300, Loss: 0.45369243919849395, Accuracy: 50.0, Precision: 0.5, Recall: 0.55\n",
            "Epoch: 0, Batch: 5310, Loss: 0.5236625105142594, Accuracy: 52.7, Precision: 0.41, Recall: 0.35\n",
            "Epoch: 0, Batch: 5320, Loss: 0.5503907382488251, Accuracy: 49.6, Precision: 0.54, Recall: 0.45\n",
            "Epoch: 0, Batch: 5330, Loss: 0.6067206174135208, Accuracy: 50.9, Precision: 0.41, Recall: 0.45\n",
            "Epoch: 0, Batch: 5340, Loss: 0.5554969102144242, Accuracy: 50.0, Precision: 0.5, Recall: 0.38\n",
            "Epoch: 0, Batch: 5350, Loss: 0.5057667583227158, Accuracy: 51.44, Precision: 0.42, Recall: 0.41\n",
            "Epoch: 0, Batch: 5360, Loss: 0.49164635986089705, Accuracy: 50.88, Precision: 0.46, Recall: 0.39\n",
            "Epoch: 0, Batch: 5370, Loss: 0.5029702186584473, Accuracy: 50.2, Precision: 0.48, Recall: 0.45\n",
            "Epoch: 0, Batch: 5380, Loss: 0.5080829352140427, Accuracy: 50.0, Precision: 0.5, Recall: 0.48\n",
            "Epoch: 0, Batch: 5390, Loss: 0.5379146248102188, Accuracy: 50.72, Precision: 0.44, Recall: 0.44\n",
            "Epoch: 0, Batch: 5400, Loss: 0.5550699770450592, Accuracy: 46.64, Precision: 0.64, Recall: 0.38\n",
            "Epoch: 0, Batch: 5410, Loss: 0.5817940652370452, Accuracy: 51.44, Precision: 0.54, Recall: 0.68\n",
            "Epoch: 0, Batch: 5420, Loss: 0.5520916759967804, Accuracy: 48.18, Precision: 0.43, Recall: 0.63\n",
            "Epoch: 0, Batch: 5430, Loss: 0.5008487820625305, Accuracy: 48.6, Precision: 0.6, Recall: 0.43\n",
            "Epoch: 0, Batch: 5440, Loss: 0.4796318501234055, Accuracy: 50.0, Precision: 0.51, Recall: 0.5\n",
            "Epoch: 0, Batch: 5450, Loss: 0.5242531388998032, Accuracy: 50.0, Precision: 0.5, Recall: 0.53\n",
            "Epoch: 0, Batch: 5460, Loss: 0.5329902589321136, Accuracy: 50.0, Precision: 0.5, Recall: 0.58\n",
            "Epoch: 0, Batch: 5470, Loss: 0.4854125589132309, Accuracy: 49.86, Precision: 0.51, Recall: 0.43\n",
            "Epoch: 0, Batch: 5480, Loss: 0.44755643904209136, Accuracy: 50.5, Precision: 0.45, Recall: 0.45\n",
            "Epoch: 0, Batch: 5490, Loss: 0.48472767472267153, Accuracy: 49.88, Precision: 0.53, Recall: 0.48\n",
            "Epoch: 0, Batch: 5500, Loss: 0.5399163961410522, Accuracy: 50.16, Precision: 0.54, Recall: 0.52\n",
            "Epoch: 0, Batch: 5510, Loss: 0.4527620166540146, Accuracy: 50.28, Precision: 0.52, Recall: 0.57\n",
            "Epoch: 0, Batch: 5520, Loss: 0.5311335057020188, Accuracy: 49.68, Precision: 0.51, Recall: 0.34\n",
            "Epoch: 0, Batch: 5530, Loss: 0.504334318637848, Accuracy: 53.08, Precision: 0.36, Recall: 0.39\n",
            "Epoch: 0, Batch: 5540, Loss: 0.5627543747425079, Accuracy: 50.72, Precision: 0.46, Recall: 0.41\n",
            "Epoch: 0, Batch: 5550, Loss: 0.4898534446954727, Accuracy: 49.68, Precision: 0.52, Recall: 0.42\n",
            "Epoch: 0, Batch: 5560, Loss: 0.4616789847612381, Accuracy: 50.0, Precision: 0.5, Recall: 0.55\n",
            "Epoch: 0, Batch: 5570, Loss: 0.5104488432407379, Accuracy: 49.84, Precision: 0.42, Recall: 0.51\n",
            "Epoch: 0, Batch: 5580, Loss: 0.42502726465463636, Accuracy: 55.52, Precision: 0.38, Recall: 0.27\n",
            "Epoch: 0, Batch: 5590, Loss: 0.6035587608814239, Accuracy: 49.6, Precision: 0.54, Recall: 0.45\n",
            "Epoch: 0, Batch: 5600, Loss: 0.5843124687671661, Accuracy: 49.52, Precision: 0.48, Recall: 0.62\n",
            "Epoch: 0, Batch: 5610, Loss: 0.46029859185218813, Accuracy: 50.3, Precision: 0.47, Recall: 0.45\n",
            "Epoch: 0, Batch: 5620, Loss: 0.4783341884613037, Accuracy: 50.0, Precision: 0.5, Recall: 0.41\n",
            "Epoch: 0, Batch: 5630, Loss: 0.5961788058280945, Accuracy: 50.16, Precision: 0.52, Recall: 0.54\n",
            "Epoch: 0, Batch: 5640, Loss: 0.4888859480619431, Accuracy: 52.0, Precision: 0.6, Recall: 0.6\n",
            "Epoch: 0, Batch: 5650, Loss: 0.5063807159662247, Accuracy: 50.0, Precision: 0.5, Recall: 0.59\n",
            "Epoch: 0, Batch: 5660, Loss: 0.5362171620130539, Accuracy: 50.08, Precision: 0.51, Recall: 0.54\n",
            "Epoch: 0, Batch: 5670, Loss: 0.47102153599262236, Accuracy: 52.04, Precision: 0.44, Recall: 0.33\n",
            "Epoch: 0, Batch: 5680, Loss: 0.5004162222146988, Accuracy: 50.4, Precision: 0.55, Recall: 0.54\n",
            "Epoch: 0, Batch: 5690, Loss: 0.5574963837862015, Accuracy: 49.92, Precision: 0.48, Recall: 0.52\n",
            "Epoch: 0, Batch: 5700, Loss: 0.4497295096516609, Accuracy: 50.08, Precision: 0.48, Recall: 0.48\n",
            "Epoch: 0, Batch: 5710, Loss: 0.47390313595533373, Accuracy: 52.8, Precision: 0.4, Recall: 0.36\n",
            "Epoch: 0, Batch: 5720, Loss: 0.5372330188751221, Accuracy: 49.44, Precision: 0.54, Recall: 0.43\n",
            "Epoch: 0, Batch: 5730, Loss: 0.46273051351308825, Accuracy: 50.32, Precision: 0.52, Recall: 0.58\n",
            "Epoch: 0, Batch: 5740, Loss: 0.48582538664340974, Accuracy: 49.94, Precision: 0.51, Recall: 0.47\n",
            "Epoch: 0, Batch: 5750, Loss: 0.5762704223394394, Accuracy: 50.16, Precision: 0.52, Recall: 0.54\n",
            "Epoch: 0, Batch: 5760, Loss: 0.5250695705413818, Accuracy: 49.96, Precision: 0.52, Recall: 0.49\n",
            "Epoch: 0, Batch: 5770, Loss: 0.5199415445327759, Accuracy: 50.0, Precision: 0.55, Recall: 0.5\n",
            "Epoch: 0, Batch: 5780, Loss: 0.5691743463277816, Accuracy: 49.72, Precision: 0.48, Recall: 0.57\n",
            "Epoch: 0, Batch: 5790, Loss: 0.5189309805631638, Accuracy: 49.92, Precision: 0.54, Recall: 0.49\n",
            "Epoch: 0, Batch: 5800, Loss: 0.48238118886947634, Accuracy: 49.88, Precision: 0.47, Recall: 0.52\n",
            "Epoch: 0, Batch: 5810, Loss: 0.5784329354763031, Accuracy: 50.14, Precision: 0.49, Recall: 0.43\n",
            "Epoch: 0, Batch: 5820, Loss: 0.5447122395038605, Accuracy: 51.12, Precision: 0.42, Recall: 0.43\n",
            "Epoch: 0, Batch: 5830, Loss: 0.5128619015216828, Accuracy: 49.52, Precision: 0.53, Recall: 0.42\n",
            "Epoch: 0, Batch: 5840, Loss: 0.5627034842967987, Accuracy: 50.1, Precision: 0.45, Recall: 0.49\n",
            "Epoch: 0, Batch: 5850, Loss: 0.5397857040166855, Accuracy: 50.8, Precision: 0.46, Recall: 0.4\n",
            "Epoch: 0, Batch: 5860, Loss: 0.5073360502719879, Accuracy: 50.3, Precision: 0.55, Recall: 0.53\n",
            "Epoch: 0, Batch: 5870, Loss: 0.4938913702964783, Accuracy: 50.3, Precision: 0.45, Recall: 0.47\n",
            "Epoch: 0, Batch: 5880, Loss: 0.5391872584819793, Accuracy: 50.0, Precision: 0.51, Recall: 0.5\n",
            "Epoch: 0, Batch: 5890, Loss: 0.4782213121652603, Accuracy: 49.86, Precision: 0.51, Recall: 0.43\n",
            "Epoch: 0, Batch: 5900, Loss: 0.5370911717414856, Accuracy: 50.1, Precision: 0.55, Recall: 0.51\n",
            "Epoch: 0, Batch: 5910, Loss: 0.5916226118803024, Accuracy: 51.82, Precision: 0.57, Recall: 0.63\n",
            "Epoch: 0, Batch: 5920, Loss: 0.46212138831615446, Accuracy: 49.92, Precision: 0.48, Recall: 0.52\n",
            "Epoch: 0, Batch: 5930, Loss: 0.43054822087287903, Accuracy: 50.14, Precision: 0.49, Recall: 0.43\n",
            "Epoch: 0, Batch: 5940, Loss: 0.5502839773893357, Accuracy: 50.66, Precision: 0.61, Recall: 0.53\n",
            "Epoch: 0, Batch: 5950, Loss: 0.5658648103475571, Accuracy: 48.98, Precision: 0.47, Recall: 0.67\n",
            "Epoch: 0, Batch: 5960, Loss: 0.5719166934490204, Accuracy: 50.0, Precision: 0.5, Recall: 0.37\n",
            "Epoch: 0, Batch: 5970, Loss: 0.4688991099596024, Accuracy: 50.64, Precision: 0.58, Recall: 0.54\n",
            "LOSS train 0.5377269983291626 valid 0.486268690940085\n"
          ]
        }
      ],
      "source": [
        "model = CNN(vocab_size,embedding_dim)\n",
        "# defining the optimizer\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "# defining the loss function\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "# checking if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    criterion = criterion.cuda()\n",
        "\n",
        "epoch_number = 0\n",
        "best_vloss = float('inf')\n",
        "\n",
        "EPOCHS = 20\n",
        "\n",
        "best_vloss = float('inf')\n",
        "train_losses = [] \n",
        "val_losses = []\n",
        "test_losses = []\n",
        "for epoch in range(EPOCHS):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "    # Make sure gradient tracking is on, and do a pass over the data\n",
        "    model.train(True)\n",
        "    avg_loss = train_epoch(epoch_number)\n",
        "    train_losses.append(avg_loss)\n",
        "\n",
        "    # Where we're going, we don't need gradients\n",
        "    model.eval() \n",
        "\n",
        "    running_vloss = 0.0\n",
        "    total_val_predictions = []\n",
        "    total_val_labels = []\n",
        "    for i, vdata in enumerate(val_loader):\n",
        "        vlabels, vinputs, offset = vdata\n",
        "        if len(vlabels) == 10:\n",
        "          vinputs = vinputs.reshape([10, 23]).to(device=device,dtype=torch.long)\n",
        "          vlabels = vlabels.float().to(device=device) \n",
        "          voutputs = model(vinputs)\n",
        "          vloss = criterion(voutputs, vlabels.unsqueeze(1))\n",
        "          running_vloss += vloss.item()\n",
        "\n",
        "          # Calculate metrics for validation set\n",
        "          rounded_vpreds = torch.round(torch.sigmoid(voutputs))\n",
        "          total_val_predictions.extend(rounded_vpreds.tolist())\n",
        "          total_val_labels.extend(vlabels.tolist())\n",
        "        else:\n",
        "          pass\n",
        "    avg_vloss = running_vloss / len(val_loader)\n",
        "    val_losses.append(avg_vloss)\n",
        "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
        "\n",
        "    val_accuracy, val_precision, val_recall = compute_metrics(torch.tensor(total_val_predictions), torch.tensor(total_val_labels))\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    val_precisions.append(val_precision)\n",
        "    val_recalls.append(val_recall)\n",
        "    # Track best performance, and save the model's state\n",
        "    if avg_vloss < best_vloss:\n",
        "        best_vloss = avg_vloss\n",
        "\n",
        "    epoch_number += 1\n",
        "\n",
        "# After all epochs, evaluate the model on the test set\n",
        "model.eval()  # Ensure the model is in evaluation mode\n",
        "running_test_loss = 0.0\n",
        "total_test_predictions = []\n",
        "total_test_labels = []\n",
        "\n",
        "for i, tdata in enumerate(test_loader):\n",
        "    tlabels, tinputs, offset = tdata\n",
        "    if len(tlabels) == 10:\n",
        "      tinputs = tinputs.reshape([10, 23]).to(device=device, dtype=torch.long)\n",
        "      tlabels = tlabels.float().to(device=device)\n",
        "      toutputs = model(tinputs)\n",
        "      tloss = criterion(toutputs, tlabels.unsqueeze(1))\n",
        "      running_test_loss += tloss.item()\n",
        "      # Calculate metrics for test set\n",
        "      rounded_tpreds = torch.round(torch.sigmoid(toutputs))\n",
        "      total_test_predictions.extend(rounded_tpreds.tolist())\n",
        "      total_test_labels.extend(tlabels.tolist())\n",
        "\n",
        "avg_test_loss = running_test_loss / len(test_loader)\n",
        "test_losses.append(avg_test_loss)\n",
        "test_accuracy, test_precision, test_recall = compute_metrics(torch.tensor(total_test_predictions), torch.tensor(total_test_labels))\n",
        "test_accuracies.append(test_accuracy)\n",
        "test_precisions.append(test_precision)\n",
        "test_recalls.append(test_recall)\n",
        "print('Test Loss: {}'.format(avg_test_loss))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "JuP30WH9Zy0X",
        "outputId": "7ae3bb8f-da8f-45ee-c53d-633dc6c53f44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "x and y must have same first dimension, but have shapes (20,) and (0,)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[66], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, EPOCHS)\n\u001b[1;32m     10\u001b[0m \u001b[39m# Plot and label the training and validation loss values\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m plt\u001b[39m.\u001b[39;49mplot(epochs, train_losses, label\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mTraining Loss\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     12\u001b[0m plt\u001b[39m.\u001b[39mplot(epochs, val_losses, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValidation Loss\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[39m# Add in a title and axes labels\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/mlclass/lib/python3.10/site-packages/matplotlib/pyplot.py:2812\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[1;32m   2811\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, scaley\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 2812\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39;49mplot(\n\u001b[1;32m   2813\u001b[0m         \u001b[39m*\u001b[39;49margs, scalex\u001b[39m=\u001b[39;49mscalex, scaley\u001b[39m=\u001b[39;49mscaley,\n\u001b[1;32m   2814\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/mlclass/lib/python3.10/site-packages/matplotlib/axes/_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[0;32m-> 1688\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[1;32m   1689\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[1;32m   1690\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/mlclass/lib/python3.10/site-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(\n\u001b[1;32m    312\u001b[0m     this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39;49mambiguous_fmt_datakey)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/mlclass/lib/python3.10/site-packages/matplotlib/axes/_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39mupdate_units(y)\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[0;32m--> 504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have same first dimension, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhave shapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    506\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    507\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y can be no greater than 2D, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (20,) and (0,)"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Generate a sequence of integers to represent the epoch numbers\n",
        "epochs = range(1, EPOCHS+1)\n",
        "\n",
        "# Plot the training and validation loss values\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_losses, label='Training Loss')\n",
        "plt.plot(epochs, val_losses, label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "# Plot the training and validation accuracy values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, train_accuracies, label='Training Accuracy')\n",
        "plt.plot(epochs, val_accuracies, label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Print test metrics\n",
        "print(\"Test Loss: {:.4f}\".format(avg_test_loss))\n",
        "print(\"Test Accuracy: {:.4f}\".format(test_accuracy))\n",
        "print(\"Test Precision: {:.4f}\".format(test_precision))\n",
        "print(\"Test Recall: {:.4f}\".format(test_recall))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a confusion matrix for the test set\n",
        "test_confusion = confusion_matrix(total_test_labels, total_test_predictions)\n",
        "plt.figure(figsize=(5, 4))\n",
        "sns.heatmap(test_confusion, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix - Test Set')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a confusion matrix for the test set\n",
        "val_confusion = confusion_matrix(total_val_labels, total_val_predictions)\n",
        "plt.figure(figsize=(5, 4))\n",
        "sns.heatmap(val_confusion, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix - Test Set')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOWit7ZZVzCpF+agTcHetCd",
      "include_colab_link": true,
      "mount_file_id": "1uYL8qjFTmqSwUOiGG1V4Nuut6-puTFf5",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
