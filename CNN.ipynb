{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wesley-Janson/transformers_for_human_vs_ai_text_identification/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sVt0Q2rzlJJs"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/mlclass/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
            "/opt/anaconda3/envs/mlclass/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Import relevant packages and data_loader.py\n",
        "#import data_loader\n",
        "\n",
        "# importing the libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# for reading and displaying images\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# for creating validation set\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# for evaluating the model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# PyTorch libraries and modules\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv1d, Conv2d, MaxPool1d, MaxPool2d, Module, Softmax, BatchNorm1d, BatchNorm2d, Dropout, Embedding\n",
        "from torch.optim import Adam, SGD\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "k8CDKMYwpjwm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aRaZIij8puzX"
      },
      "outputs": [],
      "source": [
        "def load_data(csv):\n",
        "  # Reads the raw csv file and split into\n",
        "  # sentences (x) and target (y)\n",
        "  df = pd.read_csv(csv)  \n",
        "  text = df['intro'].values\n",
        "  labels = df['type'].values\n",
        "  return labels,text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oIY1CJJ5p0zA"
      },
      "outputs": [],
      "source": [
        "\n",
        "# This function processes training data, establishing number IDs for each vocabulary word,\n",
        "# converting word sequence into ID sequence (input_as_ids), and providing dict\n",
        "# to map from word to its ID (word2id), and list to map from ID back to word (id2word)\n",
        "def process_training_data(corpus_text):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        # Create the model's vocabulary and map to unique indices\n",
        "        word2id = {}\n",
        "        id2word = []\n",
        "        indexes_dropped = []\n",
        "        list_of_inputs = []\n",
        "        for j, entry in enumerate(corpus_text):\n",
        "            for i,word in enumerate(entry):\n",
        "                if 7<i<=30:\n",
        "                    if word not in word2id:\n",
        "                        id2word.append(word)\n",
        "                        word2id[word] = len(id2word) - 1\n",
        "\n",
        "            # Convert string of text into string of IDs in a tensor for input to model\n",
        "            input_as_ids = []\n",
        "            for i,word in enumerate(entry):\n",
        "                if 7<i<=30:\n",
        "                    input_as_ids.append(word2id[word])\n",
        "            if len(input_as_ids) == 23:\n",
        "              list_of_inputs.append(input_as_ids)\n",
        "            else:\n",
        "              indexes_dropped.append(j)\n",
        "            # final_ids = torch.LongTensor(input_as_ids)\n",
        "        list_of_inputs = torch.Tensor(list_of_inputs)\n",
        "\n",
        "        return list_of_inputs,word2id,id2word, indexes_dropped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5T4irZ27ls7Z",
        "outputId": "c7e3d556-3533-4122-f0a9-2f817c8d304c"
      },
      "outputs": [],
      "source": [
        "# Run data loader\n",
        "labels, text = load_data('data/data.csv')\n",
        "train_x, val_x, train_y, val_y = train_test_split(text, labels, test_size = 0.2)\n",
        "val_x, test_x, val_y, test_y = train_test_split(val_x, val_y, test_size = 0.5)\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "list_of_tokens_train = [tokenizer(x) for x in train_x]\n",
        "list_of_tokens_val = [tokenizer(x) for x in val_x]\n",
        "list_of_tokens_test = [tokenizer(x) for x in test_x]\n",
        "\n",
        "train_x,word2id_train,id2word_train, indexes_dropped_train = process_training_data(list_of_tokens_train)\n",
        "\n",
        "val_x,word2id_val,id2word_val, indexes_dropped_val = process_training_data(list_of_tokens_val)\n",
        "\n",
        "test_x,word2id_test,id2word_test, indexes_dropped_test = process_training_data(list_of_tokens_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEao8vNKIceW",
        "outputId": "79e73750-7a42-4dd5-c535-740bb1bbd698"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([238912, 23])\n",
            "torch.Size([238912, 1])\n",
            "torch.Size([29871, 23])\n",
            "torch.Size([29871, 1])\n",
            "torch.Size([29862, 23])\n",
            "torch.Size([29862, 1])\n",
            "(238912, 24)\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "new_train = [x for x in indexes_dropped_train]\n",
        "new_val = [x for x in indexes_dropped_val]\n",
        "new_test = [x for x in indexes_dropped_test]\n",
        "\n",
        "train_y = list(train_y)\n",
        "for index, element in enumerate(new_train):\n",
        "    del train_y[element - index]  # Remove element from train_y\n",
        "    for i in range(len(indexes_dropped_train)):\n",
        "        if indexes_dropped_train[i] > element:\n",
        "            indexes_dropped_train[i] -= 1  # Decrement indexes after removal\n",
        "\n",
        "val_y = list(val_y)\n",
        "for index, element in enumerate(new_val):\n",
        "    del val_y[element - index]  # Remove element from val_y\n",
        "    for i in range(len(indexes_dropped_val)):\n",
        "        if indexes_dropped_val[i] > element:\n",
        "            indexes_dropped_val[i] -= 1  # Decrement indexes after removal\n",
        "\n",
        "test_y = list(test_y)\n",
        "for index, element in enumerate(new_test):\n",
        "    del test_y[element - index]  # Remove element from test_y\n",
        "    for i in range(len(indexes_dropped_test)):\n",
        "        if indexes_dropped_test[i] > element:\n",
        "            indexes_dropped_test[i] -= 1  # Decrement indexes after removal\n",
        "\n",
        "train_y = np.asarray(train_y)\n",
        "val_y = np.asarray(val_y)\n",
        "train_y = torch.Tensor(train_y.reshape((len(train_y), 1)))\n",
        "val_y = torch.Tensor(val_y.reshape((len(val_y), 1)))\n",
        "test_y = np.asarray(test_y)\n",
        "test_y = torch.Tensor(test_y.reshape((len(test_y), 1)))\n",
        "\n",
        "# Check the shapes of the arrays after the modifications\n",
        "print(train_x.shape)\n",
        "print(train_y.shape)\n",
        "print(val_x.shape)\n",
        "print(val_y.shape)\n",
        "print(test_x.shape)\n",
        "print(test_y.shape)\n",
        "\n",
        "text_pipeline = lambda x: x\n",
        "label_pipeline = lambda x: int(x)\n",
        "train_both = np.concatenate([train_x, train_y], axis=1)\n",
        "val_both = np.concatenate([val_x, val_y], axis=1)\n",
        "test_both = np.concatenate([test_x, test_y], axis=1)\n",
        "print(train_both.shape)\n",
        "print(type(train_both))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zQmxBERv2lg",
        "outputId": "9adc0bb9-af59-4606-f41e-b7a57d360ddc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23891 (tensor([0, 1]), tensor([    12,     32,      7,    940,     28,     30,   2181,  14617,  49437,\n",
            "          2307,    219, 182976,      0,     14,     30,   2998,     32,    675,\n",
            "            71,    131,   1558,      3, 182977,  10254,   2789,    242,     28,\n",
            "          7970,  28386,     14,     22,      6,      2,    153,     77,   6466,\n",
            "        104026,   7839,      1,  49890, 182978,  23697,      1,     12,  37614,\n",
            "          3058]), tensor([ 0, 23]))\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for entry in batch:\n",
        "         _label = entry[-1] \n",
        "         _text = entry[:len(entry)-1] \n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
        "batch_size_var = 10\n",
        "train_loader = DataLoader(train_both, batch_size=batch_size_var, shuffle=False, collate_fn=collate_batch)\n",
        "val_loader = DataLoader(train_both, batch_size=batch_size_var, shuffle=False, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_both, batch_size=batch_size_var, shuffle=False, collate_fn=collate_batch)\n",
        "trainSteps = len(train_loader.dataset) // batch_size_var\n",
        "valSteps = len(val_loader.dataset) // batch_size_var\n",
        "testSteps = len(test_loader.dataset) // batch_size_var\n",
        "for i, batch in enumerate(train_loader):\n",
        "  a,b,c = batch\n",
        "  if len(a) != 10:\n",
        "      print(i, batch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CavGDuyJxt5l",
        "outputId": "c270404b-ba35-44ff-b9d1-c6088a4bcf22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0])\n"
          ]
        }
      ],
      "source": [
        "next_, labels_, _offset = next(iter(train_loader))\n",
        "print(next_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=256, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=64, kernel_size=3, padding=1)\n",
        "\n",
        "        # Max pooling layer\n",
        "        self.max_pool = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(128, 1)  # Assuming the output size is 2\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        embedded = embedded.permute(0, 2, 1)\n",
        "        x = self.conv1(embedded)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.max_pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.max_pool(x)\n",
        "        x = self.conv3(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.max_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN(\n",
            "  (embedding): Embedding(182979, 23)\n",
            "  (conv1): Conv1d(23, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (conv2): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (conv3): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (max_pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# defining the model\n",
        "vocab_size = len(word2id_train)\n",
        "embedding_dim = 23\n",
        "\n",
        "model = CNN(vocab_size, embedding_dim)\n",
        "# defining the optimizer\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "# defining the loss function\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "# checking if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    criterion = criterion.cuda()\n",
        "    \n",
        "print(model)\n",
        "\n",
        "def compute_metrics(predictions, targets):\n",
        "    # Round predictions to get binary values\n",
        "    rounded_preds = torch.round(torch.sigmoid(predictions))\n",
        "\n",
        "    # Calculate true positives, false positives, and false negatives\n",
        "    true_positives = torch.logical_and(rounded_preds == 1, targets == 1).sum().item()\n",
        "    false_positives = torch.logical_and(rounded_preds == 1, targets == 0).sum().item()\n",
        "    false_negatives = torch.logical_and(rounded_preds == 0, targets == 1).sum().item()\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = torch.eq(rounded_preds, targets).sum().item() / targets.size(0)\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    if true_positives + false_positives > 0:\n",
        "        precision = true_positives / (true_positives + false_positives)\n",
        "    else:\n",
        "        precision = 0.0\n",
        "\n",
        "    recall = true_positives / (true_positives + false_negatives)\n",
        "\n",
        "    return accuracy, precision, recall\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7lv7xQRCx1ln"
      },
      "outputs": [],
      "source": [
        "#logistic regression bag of words, can get fwearture importance\n",
        "\n",
        "#how get feature importance in CNN's\n",
        "\n",
        "#how get tokesn out of featurs\n",
        "\n",
        "#run examples we know, print which filter getting trigegred, associate words or grams with filteere\n",
        "\n",
        "#tsney (visualize nerual net)\n",
        "\n",
        "#take examples human got wrong, see what we get right, find k-grams that ar enon-triavail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_accuracies = []\n",
        "train_precisions = []\n",
        "train_recalls = []\n",
        "val_accuracies = []\n",
        "val_precisions = []\n",
        "val_recalls = []\n",
        "test_accuracies = []\n",
        "test_precisions = []\n",
        "test_recalls = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(epoch):\n",
        "    running_loss = 0.0\n",
        "    total_predictions = []\n",
        "    total_labels = []\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "        labels, inputs, offset = data\n",
        "        if len(labels) == 10:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            inputs = inputs.reshape([10, 23]).to(device=device, dtype=torch.long)\n",
        "            labels = labels.float().to(device=device)  # Convert labels to float\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            loss = criterion(outputs, labels.unsqueeze(1))\n",
        "\n",
        "            # Calculate metrics\n",
        "            rounded_preds = torch.round(torch.sigmoid(outputs))\n",
        "            total_predictions.extend(rounded_preds.tolist())\n",
        "            total_labels.extend(labels.tolist())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % 10 == 9:  # Print average loss every 10 batches\n",
        "                average_loss = running_loss / 10\n",
        "                accuracy, precision, recall = compute_metrics(torch.tensor(total_predictions), torch.tensor(total_labels))\n",
        "                print(f\"Epoch: {epoch}, Batch: {i+1}, Loss: {average_loss}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}\")\n",
        "                running_loss = 0.0\n",
        "\n",
        "                # Reset lists for next batch\n",
        "                total_predictions = []\n",
        "                total_labels = []\n",
        "\n",
        "    return running_loss\n",
        "def train_epoch(epoch):\n",
        "    running_loss = 0.0\n",
        "    total_predictions = []\n",
        "    total_labels = []\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "        labels, inputs, offset = data\n",
        "        if len(labels) == 10:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            inputs = inputs.reshape([10, 23]).to(device=device, dtype=torch.long)\n",
        "            labels = labels.float().to(device=device)  # Convert labels to float\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            loss = criterion(outputs, labels.unsqueeze(1))\n",
        "\n",
        "            # Calculate metrics\n",
        "            rounded_preds = torch.round(torch.sigmoid(outputs))\n",
        "            total_predictions.extend(rounded_preds.tolist())\n",
        "            total_labels.extend(labels.tolist())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % 10 == 9:  # Print average loss every 10 batches\n",
        "                average_loss = running_loss / 10\n",
        "                accuracy, precision, recall = compute_metrics(torch.tensor(total_predictions), torch.tensor(total_labels))\n",
        "                print(f\"Epoch: {epoch}, Batch: {i+1}, Loss: {average_loss}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}\")\n",
        "                running_loss = 0.0\n",
        "\n",
        "                # Reset lists for next batch\n",
        "                total_predictions = []\n",
        "                total_labels = []\n",
        "\n",
        "    return running_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWUhv7b-WJMk",
        "outputId": "bd044904-19f2-4022-b23f-4974982c3188"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 1:\n",
            "Epoch: 0, Batch: 10, Loss: 0.7020741879940033, Accuracy: 51.84, Precision: 0.54, Recall: 0.73\n",
            "Epoch: 0, Batch: 20, Loss: 0.6998058736324311, Accuracy: 49.58, Precision: 0.43, Recall: 0.53\n",
            "Epoch: 0, Batch: 30, Loss: 0.70354323387146, Accuracy: 52.0, Precision: 0.0, Recall: 0.0\n",
            "Epoch: 0, Batch: 40, Loss: 0.6966799974441529, Accuracy: 50.08, Precision: 0.52, Recall: 0.52\n",
            "Epoch: 0, Batch: 50, Loss: 0.6953270077705384, Accuracy: 48.8, Precision: 0.48, Recall: 0.8\n",
            "Epoch: 0, Batch: 60, Loss: 0.6933410465717316, Accuracy: 50.0, Precision: 0.5, Recall: 0.54\n",
            "Epoch: 0, Batch: 70, Loss: 0.6931746304035187, Accuracy: 50.36, Precision: 0.48, Recall: 0.41\n",
            "Epoch: 0, Batch: 80, Loss: 0.693159306049347, Accuracy: 49.04, Precision: 0.52, Recall: 0.26\n",
            "Epoch: 0, Batch: 90, Loss: 0.6965607166290283, Accuracy: 56.72, Precision: 0.42, Recall: 0.08\n",
            "Epoch: 0, Batch: 100, Loss: 0.6914930880069733, Accuracy: 52.0, Precision: 0.0, Recall: 0.0\n",
            "Epoch: 0, Batch: 110, Loss: 0.6969154179096222, Accuracy: 46.08, Precision: 0.54, Recall: 0.01\n",
            "Epoch: 0, Batch: 120, Loss: 0.6915716469287873, Accuracy: 50.92, Precision: 0.49, Recall: 0.04\n",
            "Epoch: 0, Batch: 130, Loss: 0.6919218480587006, Accuracy: 50.18, Precision: 0.49, Recall: 0.41\n",
            "Epoch: 0, Batch: 140, Loss: 0.6903610229492188, Accuracy: 53.0, Precision: 0.6, Recall: 0.65\n",
            "Epoch: 0, Batch: 150, Loss: 0.6942828834056854, Accuracy: 51.0, Precision: 0.51, Recall: 1.0\n",
            "Epoch: 0, Batch: 160, Loss: 0.6945626080036164, Accuracy: 53.92, Precision: 0.54, Recall: 0.99\n",
            "Epoch: 0, Batch: 170, Loss: 0.6923183500766754, Accuracy: 51.84, Precision: 0.52, Recall: 0.96\n",
            "Epoch: 0, Batch: 180, Loss: 0.6959821045398712, Accuracy: 51.08, Precision: 0.53, Recall: 0.68\n",
            "Epoch: 0, Batch: 190, Loss: 0.6942486107349396, Accuracy: 48.96, Precision: 0.48, Recall: 0.76\n",
            "Epoch: 0, Batch: 200, Loss: 0.6863811671733856, Accuracy: 51.56, Precision: 0.56, Recall: 0.63\n",
            "Epoch: 0, Batch: 210, Loss: 0.6869642198085785, Accuracy: 54.7, Precision: 0.55, Recall: 0.97\n",
            "Epoch: 0, Batch: 220, Loss: 0.6917484343051911, Accuracy: 51.0, Precision: 0.51, Recall: 1.0\n",
            "Epoch: 0, Batch: 230, Loss: 0.6843543827533722, Accuracy: 52.8, Precision: 0.54, Recall: 0.85\n",
            "Epoch: 0, Batch: 240, Loss: 0.691570496559143, Accuracy: 50.0, Precision: 0.5, Recall: 0.9\n",
            "Epoch: 0, Batch: 250, Loss: 0.6919062435626984, Accuracy: 50.0, Precision: 0.5, Recall: 0.77\n",
            "Epoch: 0, Batch: 260, Loss: 0.6896624267101288, Accuracy: 49.44, Precision: 0.49, Recall: 0.78\n",
            "Epoch: 0, Batch: 270, Loss: 0.6911536872386932, Accuracy: 50.98, Precision: 0.43, Recall: 0.43\n",
            "Epoch: 0, Batch: 280, Loss: 0.693055534362793, Accuracy: 51.88, Precision: 0.48, Recall: 0.03\n",
            "Epoch: 0, Batch: 290, Loss: 0.6924242198467254, Accuracy: 48.56, Precision: 0.53, Recall: 0.26\n",
            "Epoch: 0, Batch: 300, Loss: 0.6843474686145783, Accuracy: 54.0, Precision: 0.55, Recall: 0.9\n",
            "Epoch: 0, Batch: 310, Loss: 0.7089516818523407, Accuracy: 44.72, Precision: 0.39, Recall: 0.74\n",
            "Epoch: 0, Batch: 320, Loss: 0.6956824421882629, Accuracy: 48.6, Precision: 0.52, Recall: 0.15\n",
            "Epoch: 0, Batch: 330, Loss: 0.6923149764537812, Accuracy: 48.2, Precision: 0.53, Recall: 0.2\n",
            "Epoch: 0, Batch: 340, Loss: 0.6915371239185333, Accuracy: 50.24, Precision: 0.51, Recall: 0.62\n",
            "Epoch: 0, Batch: 350, Loss: 0.6847660839557648, Accuracy: 51.74, Precision: 0.53, Recall: 0.79\n",
            "Epoch: 0, Batch: 360, Loss: 0.6735930025577546, Accuracy: 54.0, Precision: 0.54, Recall: 1.0\n",
            "Epoch: 0, Batch: 370, Loss: 0.6762492597103119, Accuracy: 50.0, Precision: 0.5, Recall: 0.63\n",
            "Epoch: 0, Batch: 380, Loss: 0.6530925273895264, Accuracy: 58.84, Precision: 0.33, Recall: 0.24\n",
            "Epoch: 0, Batch: 390, Loss: 0.6696048378944397, Accuracy: 52.24, Precision: 0.46, Recall: 0.22\n",
            "Epoch: 0, Batch: 400, Loss: 0.6738529324531555, Accuracy: 51.8, Precision: 0.55, Recall: 0.68\n",
            "Epoch: 0, Batch: 410, Loss: 0.6204438209533691, Accuracy: 51.68, Precision: 0.56, Recall: 0.64\n",
            "Epoch: 0, Batch: 420, Loss: 0.6779098868370056, Accuracy: 52.8, Precision: 0.4, Recall: 0.36\n",
            "Epoch: 0, Batch: 430, Loss: 0.6554236233234405, Accuracy: 56.56, Precision: 0.42, Recall: 0.09\n",
            "Epoch: 0, Batch: 440, Loss: 0.666333693265915, Accuracy: 50.0, Precision: 0.5, Recall: 0.56\n",
            "Epoch: 0, Batch: 450, Loss: 0.644238555431366, Accuracy: 50.06, Precision: 0.49, Recall: 0.47\n",
            "Epoch: 0, Batch: 460, Loss: 0.6122480422258377, Accuracy: 59.24, Precision: 0.64, Recall: 0.83\n",
            "Epoch: 0, Batch: 470, Loss: 0.6793928742408752, Accuracy: 50.56, Precision: 0.54, Recall: 0.57\n",
            "Epoch: 0, Batch: 480, Loss: 0.6697191894054413, Accuracy: 49.28, Precision: 0.53, Recall: 0.38\n",
            "Epoch: 0, Batch: 490, Loss: 0.6343241989612579, Accuracy: 51.38, Precision: 0.53, Recall: 0.73\n",
            "Epoch: 0, Batch: 500, Loss: 0.7231992244720459, Accuracy: 49.92, Precision: 0.49, Recall: 0.54\n",
            "Epoch: 0, Batch: 510, Loss: 0.659042376279831, Accuracy: 49.82, Precision: 0.51, Recall: 0.41\n",
            "Epoch: 0, Batch: 520, Loss: 0.6370021164417267, Accuracy: 50.08, Precision: 0.54, Recall: 0.51\n",
            "Epoch: 0, Batch: 530, Loss: 0.6867631852626801, Accuracy: 47.24, Precision: 0.44, Recall: 0.73\n",
            "Epoch: 0, Batch: 540, Loss: 0.6661100447177887, Accuracy: 50.72, Precision: 0.48, Recall: 0.32\n",
            "Epoch: 0, Batch: 550, Loss: 0.6501022040843963, Accuracy: 49.44, Precision: 0.51, Recall: 0.22\n",
            "Epoch: 0, Batch: 560, Loss: 0.6707343459129333, Accuracy: 49.6, Precision: 0.4, Recall: 0.52\n",
            "Epoch: 0, Batch: 570, Loss: 0.6579852998256683, Accuracy: 50.38, Precision: 0.49, Recall: 0.31\n",
            "Epoch: 0, Batch: 580, Loss: 0.6160143375396728, Accuracy: 50.0, Precision: 0.5, Recall: 0.53\n",
            "Epoch: 0, Batch: 590, Loss: 0.6365357756614685, Accuracy: 52.56, Precision: 0.58, Recall: 0.66\n",
            "Epoch: 0, Batch: 600, Loss: 0.6611931443214416, Accuracy: 50.34, Precision: 0.51, Recall: 0.67\n",
            "Epoch: 0, Batch: 610, Loss: 0.6298340439796448, Accuracy: 51.08, Precision: 0.47, Recall: 0.32\n",
            "Epoch: 0, Batch: 620, Loss: 0.7031151473522186, Accuracy: 49.28, Precision: 0.54, Recall: 0.41\n",
            "Epoch: 0, Batch: 630, Loss: 0.6491305232048035, Accuracy: 51.12, Precision: 0.52, Recall: 0.78\n",
            "Epoch: 0, Batch: 640, Loss: 0.6125928103923798, Accuracy: 49.52, Precision: 0.48, Recall: 0.62\n",
            "Epoch: 0, Batch: 650, Loss: 0.6213252186775208, Accuracy: 50.18, Precision: 0.47, Recall: 0.47\n",
            "Epoch: 0, Batch: 660, Loss: 0.5990419089794159, Accuracy: 49.56, Precision: 0.52, Recall: 0.39\n",
            "Epoch: 0, Batch: 670, Loss: 0.6206109911203385, Accuracy: 50.0, Precision: 0.5, Recall: 0.49\n",
            "Epoch: 0, Batch: 680, Loss: 0.6260620057582855, Accuracy: 53.12, Precision: 0.44, Recall: 0.24\n",
            "Epoch: 0, Batch: 690, Loss: 0.6106151729822159, Accuracy: 50.4, Precision: 0.52, Recall: 0.6\n",
            "Epoch: 0, Batch: 700, Loss: 0.6471804976463318, Accuracy: 51.02, Precision: 0.47, Recall: 0.33\n",
            "Epoch: 0, Batch: 710, Loss: 0.6677995145320892, Accuracy: 51.32, Precision: 0.48, Recall: 0.17\n",
            "Epoch: 0, Batch: 720, Loss: 0.6656539678573609, Accuracy: 49.86, Precision: 0.51, Recall: 0.43\n",
            "Epoch: 0, Batch: 730, Loss: 0.6219702184200286, Accuracy: 49.64, Precision: 0.53, Recall: 0.44\n",
            "Epoch: 0, Batch: 740, Loss: 0.6376672148704529, Accuracy: 49.22, Precision: 0.53, Recall: 0.37\n",
            "Epoch: 0, Batch: 750, Loss: 0.6127892911434174, Accuracy: 50.0, Precision: 0.53, Recall: 0.5\n",
            "Epoch: 0, Batch: 760, Loss: 0.6497042596340179, Accuracy: 51.6, Precision: 0.45, Recall: 0.34\n",
            "Epoch: 0, Batch: 770, Loss: 0.6434692442417145, Accuracy: 51.86, Precision: 0.47, Recall: 0.19\n",
            "Epoch: 0, Batch: 780, Loss: 0.6705909371376038, Accuracy: 51.3, Precision: 0.55, Recall: 0.63\n",
            "Epoch: 0, Batch: 790, Loss: 0.6750655770301819, Accuracy: 50.2, Precision: 0.55, Recall: 0.52\n",
            "Epoch: 0, Batch: 800, Loss: 0.6307483971118927, Accuracy: 51.44, Precision: 0.46, Recall: 0.32\n",
            "Epoch: 0, Batch: 810, Loss: 0.6431510597467422, Accuracy: 50.3, Precision: 0.49, Recall: 0.35\n",
            "Epoch: 0, Batch: 820, Loss: 0.6018870830535888, Accuracy: 50.84, Precision: 0.48, Recall: 0.29\n",
            "Epoch: 0, Batch: 830, Loss: 0.5653733313083649, Accuracy: 50.32, Precision: 0.54, Recall: 0.54\n",
            "Epoch: 0, Batch: 840, Loss: 0.6766268253326416, Accuracy: 49.84, Precision: 0.48, Recall: 0.54\n",
            "Epoch: 0, Batch: 850, Loss: 0.6183002471923829, Accuracy: 53.48, Precision: 0.44, Recall: 0.21\n",
            "Epoch: 0, Batch: 860, Loss: 0.6723816871643067, Accuracy: 50.5, Precision: 0.49, Recall: 0.25\n",
            "Epoch: 0, Batch: 870, Loss: 0.652094841003418, Accuracy: 50.0, Precision: 0.5, Recall: 0.59\n",
            "Epoch: 0, Batch: 880, Loss: 0.6153343677520752, Accuracy: 51.36, Precision: 0.46, Recall: 0.33\n",
            "Epoch: 0, Batch: 890, Loss: 0.6064557939767837, Accuracy: 50.0, Precision: 0.58, Recall: 0.5\n",
            "Epoch: 0, Batch: 900, Loss: 0.6573814630508423, Accuracy: 51.0, Precision: 0.55, Recall: 0.6\n",
            "Epoch: 0, Batch: 910, Loss: 0.5669815450906753, Accuracy: 49.58, Precision: 0.53, Recall: 0.43\n",
            "Epoch: 0, Batch: 920, Loss: 0.6295044213533402, Accuracy: 52.24, Precision: 0.42, Recall: 0.36\n",
            "Epoch: 0, Batch: 930, Loss: 0.6379627317190171, Accuracy: 50.0, Precision: 0.5, Recall: 0.31\n",
            "Epoch: 0, Batch: 940, Loss: 0.5870597660541534, Accuracy: 50.7, Precision: 0.55, Recall: 0.57\n",
            "Epoch: 0, Batch: 950, Loss: 0.5842019021511078, Accuracy: 49.88, Precision: 0.48, Recall: 0.53\n",
            "Epoch: 0, Batch: 960, Loss: 0.6614009290933609, Accuracy: 50.32, Precision: 0.49, Recall: 0.34\n",
            "Epoch: 0, Batch: 970, Loss: 0.6251686215400696, Accuracy: 50.0, Precision: 0.5, Recall: 0.52\n",
            "Epoch: 0, Batch: 980, Loss: 0.653652822971344, Accuracy: 50.16, Precision: 0.42, Recall: 0.49\n",
            "Epoch: 0, Batch: 990, Loss: 0.6157500147819519, Accuracy: 53.4, Precision: 0.45, Recall: 0.16\n",
            "Epoch: 0, Batch: 1000, Loss: 0.5825298607349396, Accuracy: 50.2, Precision: 0.6, Recall: 0.51\n",
            "Epoch: 0, Batch: 1010, Loss: 0.7150124728679657, Accuracy: 51.84, Precision: 0.54, Recall: 0.73\n",
            "Epoch: 0, Batch: 1020, Loss: 0.5963820964097977, Accuracy: 50.28, Precision: 0.43, Recall: 0.48\n",
            "Epoch: 0, Batch: 1030, Loss: 0.6402061700820922, Accuracy: 49.36, Precision: 0.54, Recall: 0.42\n",
            "Epoch: 0, Batch: 1040, Loss: 0.5876780241727829, Accuracy: 50.96, Precision: 0.44, Recall: 0.42\n",
            "Epoch: 0, Batch: 1050, Loss: 0.6299093455076218, Accuracy: 50.0, Precision: 0.5, Recall: 0.41\n",
            "Epoch: 0, Batch: 1060, Loss: 0.5985236525535583, Accuracy: 50.48, Precision: 0.46, Recall: 0.44\n",
            "Epoch: 0, Batch: 1070, Loss: 0.611289981007576, Accuracy: 53.36, Precision: 0.42, Recall: 0.29\n",
            "Epoch: 0, Batch: 1080, Loss: 0.6122800290584565, Accuracy: 49.56, Precision: 0.52, Recall: 0.39\n",
            "Epoch: 0, Batch: 1090, Loss: 0.6549349665641785, Accuracy: 49.64, Precision: 0.44, Recall: 0.53\n",
            "Epoch: 0, Batch: 1100, Loss: 0.6279990375041962, Accuracy: 51.7, Precision: 0.45, Recall: 0.33\n",
            "Epoch: 0, Batch: 1110, Loss: 0.5815880566835403, Accuracy: 50.24, Precision: 0.49, Recall: 0.38\n",
            "Epoch: 0, Batch: 1120, Loss: 0.6267470091581344, Accuracy: 50.42, Precision: 0.53, Recall: 0.57\n",
            "Epoch: 0, Batch: 1130, Loss: 0.6168828278779983, Accuracy: 51.6, Precision: 0.4, Recall: 0.42\n",
            "Epoch: 0, Batch: 1140, Loss: 0.6031547367572785, Accuracy: 49.1, Precision: 0.55, Recall: 0.41\n",
            "Epoch: 0, Batch: 1150, Loss: 0.7197373420000076, Accuracy: 50.12, Precision: 0.51, Recall: 0.56\n",
            "Epoch: 0, Batch: 1160, Loss: 0.5893236815929412, Accuracy: 50.04, Precision: 0.49, Recall: 0.48\n",
            "Epoch: 0, Batch: 1170, Loss: 0.6341364622116089, Accuracy: 50.64, Precision: 0.46, Recall: 0.42\n",
            "Epoch: 0, Batch: 1180, Loss: 0.578765144944191, Accuracy: 49.86, Precision: 0.51, Recall: 0.43\n",
            "Epoch: 0, Batch: 1190, Loss: 0.6650451600551606, Accuracy: 49.94, Precision: 0.51, Recall: 0.47\n",
            "Epoch: 0, Batch: 1200, Loss: 0.6004523903131485, Accuracy: 52.64, Precision: 0.39, Recall: 0.38\n",
            "Epoch: 0, Batch: 1210, Loss: 0.5959017902612687, Accuracy: 50.0, Precision: 0.5, Recall: 0.32\n",
            "Epoch: 0, Batch: 1220, Loss: 0.6097481459379196, Accuracy: 50.0, Precision: 0.54, Recall: 0.5\n",
            "Epoch: 0, Batch: 1230, Loss: 0.6229568809270859, Accuracy: 50.0, Precision: 0.5, Recall: 0.52\n",
            "Epoch: 0, Batch: 1240, Loss: 0.5700879245996475, Accuracy: 50.36, Precision: 0.47, Recall: 0.44\n",
            "Epoch: 0, Batch: 1250, Loss: 0.5802424430847168, Accuracy: 50.84, Precision: 0.47, Recall: 0.36\n",
            "Epoch: 0, Batch: 1260, Loss: 0.5517285287380218, Accuracy: 50.5, Precision: 0.55, Recall: 0.55\n",
            "Epoch: 0, Batch: 1270, Loss: 0.6268911242485047, Accuracy: 49.64, Precision: 0.48, Recall: 0.59\n",
            "Epoch: 0, Batch: 1280, Loss: 0.6329769343137741, Accuracy: 50.42, Precision: 0.49, Recall: 0.29\n",
            "Epoch: 0, Batch: 1290, Loss: 0.626999917626381, Accuracy: 50.32, Precision: 0.46, Recall: 0.46\n",
            "Epoch: 0, Batch: 1300, Loss: 0.6580977767705918, Accuracy: 50.06, Precision: 0.49, Recall: 0.47\n",
            "Epoch: 0, Batch: 1310, Loss: 0.5853640854358673, Accuracy: 49.28, Precision: 0.56, Recall: 0.44\n",
            "Epoch: 0, Batch: 1320, Loss: 0.6481115520000458, Accuracy: 50.78, Precision: 0.53, Recall: 0.63\n",
            "Epoch: 0, Batch: 1330, Loss: 0.5509303867816925, Accuracy: 50.18, Precision: 0.53, Recall: 0.53\n",
            "Epoch: 0, Batch: 1340, Loss: 0.5849961668252945, Accuracy: 49.92, Precision: 0.46, Recall: 0.51\n",
            "Epoch: 0, Batch: 1350, Loss: 0.621814900636673, Accuracy: 50.16, Precision: 0.49, Recall: 0.42\n",
            "Epoch: 0, Batch: 1360, Loss: 0.5716056555509568, Accuracy: 50.0, Precision: 0.5, Recall: 0.42\n",
            "Epoch: 0, Batch: 1370, Loss: 0.6060872733592987, Accuracy: 50.12, Precision: 0.49, Recall: 0.44\n",
            "Epoch: 0, Batch: 1380, Loss: 0.6171036243438721, Accuracy: 50.32, Precision: 0.54, Recall: 0.54\n",
            "Epoch: 0, Batch: 1390, Loss: 0.61211456656456, Accuracy: 50.0, Precision: 0.46, Recall: 0.5\n",
            "Epoch: 0, Batch: 1400, Loss: 0.6557743221521377, Accuracy: 50.0, Precision: 0.5, Recall: 0.38\n",
            "Epoch: 0, Batch: 1410, Loss: 0.5795044243335724, Accuracy: 50.06, Precision: 0.49, Recall: 0.47\n",
            "Epoch: 0, Batch: 1420, Loss: 0.6043730020523072, Accuracy: 49.6, Precision: 0.46, Recall: 0.55\n",
            "Epoch: 0, Batch: 1430, Loss: 0.6137946128845215, Accuracy: 50.2, Precision: 0.49, Recall: 0.4\n",
            "Epoch: 0, Batch: 1440, Loss: 0.575811293721199, Accuracy: 49.58, Precision: 0.53, Recall: 0.43\n",
            "Epoch: 0, Batch: 1450, Loss: 0.5941393196582794, Accuracy: 49.64, Precision: 0.44, Recall: 0.53\n",
            "Epoch: 0, Batch: 1460, Loss: 0.5741213738918305, Accuracy: 49.68, Precision: 0.52, Recall: 0.42\n",
            "Epoch: 0, Batch: 1470, Loss: 0.5450667709112167, Accuracy: 50.28, Precision: 0.57, Recall: 0.52\n",
            "Epoch: 0, Batch: 1480, Loss: 0.622374826669693, Accuracy: 49.52, Precision: 0.48, Recall: 0.62\n",
            "Epoch: 0, Batch: 1490, Loss: 0.6130160748958587, Accuracy: 49.46, Precision: 0.53, Recall: 0.41\n",
            "Epoch: 0, Batch: 1500, Loss: 0.5613382816314697, Accuracy: 50.24, Precision: 0.47, Recall: 0.46\n",
            "Epoch: 0, Batch: 1510, Loss: 0.5938510507345199, Accuracy: 50.6, Precision: 0.45, Recall: 0.44\n",
            "Epoch: 0, Batch: 1520, Loss: 0.5936102449893952, Accuracy: 51.44, Precision: 0.42, Recall: 0.41\n",
            "Epoch: 0, Batch: 1530, Loss: 0.5463647961616516, Accuracy: 50.92, Precision: 0.48, Recall: 0.27\n",
            "Epoch: 0, Batch: 1540, Loss: 0.6712945371866226, Accuracy: 48.72, Precision: 0.46, Recall: 0.66\n",
            "Epoch: 0, Batch: 1550, Loss: 0.5421578109264373, Accuracy: 50.0, Precision: 0.5, Recall: 0.36\n",
            "Epoch: 0, Batch: 1560, Loss: 0.5702613145112991, Accuracy: 49.92, Precision: 0.52, Recall: 0.48\n",
            "Epoch: 0, Batch: 1570, Loss: 0.506029212474823, Accuracy: 50.4, Precision: 0.52, Recall: 0.6\n",
            "Epoch: 0, Batch: 1580, Loss: 0.5663165271282196, Accuracy: 51.32, Precision: 0.44, Recall: 0.39\n",
            "Epoch: 0, Batch: 1590, Loss: 0.5151415050029755, Accuracy: 50.0, Precision: 0.46, Recall: 0.5\n",
            "Epoch: 0, Batch: 1600, Loss: 0.6362356275320054, Accuracy: 49.78, Precision: 0.51, Recall: 0.39\n",
            "Epoch: 0, Batch: 1610, Loss: 0.6311804324388504, Accuracy: 50.22, Precision: 0.51, Recall: 0.61\n",
            "Epoch: 0, Batch: 1620, Loss: 0.578399658203125, Accuracy: 51.02, Precision: 0.47, Recall: 0.33\n",
            "Epoch: 0, Batch: 1630, Loss: 0.5349593818187713, Accuracy: 49.7, Precision: 0.53, Recall: 0.45\n",
            "Epoch: 0, Batch: 1640, Loss: 0.7767381489276886, Accuracy: 49.36, Precision: 0.48, Recall: 0.66\n",
            "Epoch: 0, Batch: 1650, Loss: 0.5980170965194702, Accuracy: 50.9, Precision: 0.47, Recall: 0.35\n",
            "Epoch: 0, Batch: 1660, Loss: 0.5949662148952484, Accuracy: 52.94, Precision: 0.43, Recall: 0.29\n",
            "Epoch: 0, Batch: 1670, Loss: 0.5594506591558457, Accuracy: 50.88, Precision: 0.46, Recall: 0.39\n",
            "Epoch: 0, Batch: 1680, Loss: 0.6366548657417297, Accuracy: 51.6, Precision: 0.58, Recall: 0.6\n",
            "Epoch: 0, Batch: 1690, Loss: 0.5652190327644349, Accuracy: 50.24, Precision: 0.46, Recall: 0.47\n",
            "Epoch: 0, Batch: 1700, Loss: 0.5910500377416611, Accuracy: 52.4, Precision: 0.45, Recall: 0.26\n",
            "Epoch: 0, Batch: 1710, Loss: 0.5477235287427902, Accuracy: 50.48, Precision: 0.48, Recall: 0.38\n",
            "Epoch: 0, Batch: 1720, Loss: 0.6313701748847962, Accuracy: 50.0, Precision: 0.51, Recall: 0.5\n",
            "Epoch: 0, Batch: 1730, Loss: 0.5685928732156753, Accuracy: 51.68, Precision: 0.56, Recall: 0.64\n",
            "Epoch: 0, Batch: 1740, Loss: 0.561193260550499, Accuracy: 50.6, Precision: 0.44, Recall: 0.45\n",
            "Epoch: 0, Batch: 1750, Loss: 0.7097097277641297, Accuracy: 49.56, Precision: 0.51, Recall: 0.28\n",
            "Epoch: 0, Batch: 1760, Loss: 0.6070776760578156, Accuracy: 50.06, Precision: 0.49, Recall: 0.47\n",
            "Epoch: 0, Batch: 1770, Loss: 0.5453171521425247, Accuracy: 50.08, Precision: 0.52, Recall: 0.52\n",
            "Epoch: 0, Batch: 1780, Loss: 0.6088070273399353, Accuracy: 49.76, Precision: 0.49, Recall: 0.62\n",
            "Epoch: 0, Batch: 1790, Loss: 0.6291312009096146, Accuracy: 49.84, Precision: 0.51, Recall: 0.42\n",
            "Epoch: 0, Batch: 1800, Loss: 0.5773008376359939, Accuracy: 49.8, Precision: 0.52, Recall: 0.45\n",
            "Epoch: 0, Batch: 1810, Loss: 0.6290554612874985, Accuracy: 50.56, Precision: 0.43, Recall: 0.46\n",
            "Epoch: 0, Batch: 1820, Loss: 0.5545758724212646, Accuracy: 53.42, Precision: 0.41, Recall: 0.31\n",
            "Epoch: 0, Batch: 1830, Loss: 0.6447688907384872, Accuracy: 51.7, Precision: 0.45, Recall: 0.33\n",
            "Epoch: 0, Batch: 1840, Loss: 0.5431463956832886, Accuracy: 51.12, Precision: 0.46, Recall: 0.36\n",
            "Epoch: 0, Batch: 1850, Loss: 0.5782934844493866, Accuracy: 50.08, Precision: 0.52, Recall: 0.52\n",
            "Epoch: 0, Batch: 1860, Loss: 0.6258912712335587, Accuracy: 50.08, Precision: 0.46, Recall: 0.49\n",
            "Epoch: 0, Batch: 1870, Loss: 0.5722823292016983, Accuracy: 50.4, Precision: 0.48, Recall: 0.4\n",
            "Epoch: 0, Batch: 1880, Loss: 0.5396036118268966, Accuracy: 50.6, Precision: 0.48, Recall: 0.35\n",
            "Epoch: 0, Batch: 1890, Loss: 0.6210374623537064, Accuracy: 50.6, Precision: 0.45, Recall: 0.44\n",
            "Epoch: 0, Batch: 1900, Loss: 0.6316388040781021, Accuracy: 50.22, Precision: 0.49, Recall: 0.39\n",
            "Epoch: 0, Batch: 1910, Loss: 0.6357801377773284, Accuracy: 50.56, Precision: 0.48, Recall: 0.36\n",
            "Epoch: 0, Batch: 1920, Loss: 0.6314828038215637, Accuracy: 50.04, Precision: 0.49, Recall: 0.48\n",
            "Epoch: 0, Batch: 1930, Loss: 0.5397207170724869, Accuracy: 50.32, Precision: 0.48, Recall: 0.42\n",
            "Epoch: 0, Batch: 1940, Loss: 0.605765038728714, Accuracy: 50.08, Precision: 0.54, Recall: 0.51\n",
            "Epoch: 0, Batch: 1950, Loss: 0.5899111032485962, Accuracy: 50.24, Precision: 0.54, Recall: 0.53\n",
            "Epoch: 0, Batch: 1960, Loss: 0.6320167273283005, Accuracy: 49.72, Precision: 0.48, Recall: 0.57\n",
            "Epoch: 0, Batch: 1970, Loss: 0.5272779345512391, Accuracy: 51.2, Precision: 0.46, Recall: 0.35\n",
            "Epoch: 0, Batch: 1980, Loss: 0.5919874668121338, Accuracy: 50.0, Precision: 0.55, Recall: 0.5\n",
            "Epoch: 0, Batch: 1990, Loss: 0.5912538051605225, Accuracy: 50.8, Precision: 0.54, Recall: 0.6\n",
            "Epoch: 0, Batch: 2000, Loss: 0.636773806810379, Accuracy: 49.96, Precision: 0.51, Recall: 0.48\n",
            "Epoch: 0, Batch: 2010, Loss: 0.5595978736877442, Accuracy: 50.36, Precision: 0.48, Recall: 0.41\n",
            "Epoch: 0, Batch: 2020, Loss: 0.5437806934118271, Accuracy: 50.14, Precision: 0.43, Recall: 0.49\n",
            "Epoch: 0, Batch: 2030, Loss: 0.5961437165737152, Accuracy: 51.28, Precision: 0.58, Recall: 0.58\n",
            "Epoch: 0, Batch: 2040, Loss: 0.5926461488008499, Accuracy: 49.6, Precision: 0.48, Recall: 0.6\n",
            "Epoch: 0, Batch: 2050, Loss: 0.5316157460212707, Accuracy: 49.8, Precision: 0.6, Recall: 0.49\n",
            "Epoch: 0, Batch: 2060, Loss: 0.628018257021904, Accuracy: 48.56, Precision: 0.41, Recall: 0.58\n",
            "Epoch: 0, Batch: 2070, Loss: 0.6140491485595703, Accuracy: 49.56, Precision: 0.52, Recall: 0.39\n",
            "Epoch: 0, Batch: 2080, Loss: 0.5553565949201584, Accuracy: 50.12, Precision: 0.53, Recall: 0.52\n",
            "Epoch: 0, Batch: 2090, Loss: 0.5309246838092804, Accuracy: 50.28, Precision: 0.52, Recall: 0.57\n",
            "Epoch: 0, Batch: 2100, Loss: 0.6077976852655411, Accuracy: 50.36, Precision: 0.56, Recall: 0.53\n",
            "Epoch: 0, Batch: 2110, Loss: 0.6269043385982513, Accuracy: 49.28, Precision: 0.44, Recall: 0.56\n",
            "Epoch: 0, Batch: 2120, Loss: 0.6544649451971054, Accuracy: 48.92, Precision: 0.53, Recall: 0.32\n",
            "Epoch: 0, Batch: 2130, Loss: 0.5446774899959564, Accuracy: 50.2, Precision: 0.45, Recall: 0.48\n",
            "Epoch: 0, Batch: 2140, Loss: 0.6053823828697205, Accuracy: 50.6, Precision: 0.44, Recall: 0.45\n",
            "Epoch: 0, Batch: 2150, Loss: 0.61995709836483, Accuracy: 52.72, Precision: 0.42, Recall: 0.33\n",
            "Epoch: 0, Batch: 2160, Loss: 0.589380931854248, Accuracy: 49.44, Precision: 0.52, Recall: 0.36\n",
            "Epoch: 0, Batch: 2170, Loss: 0.6063277840614318, Accuracy: 50.42, Precision: 0.57, Recall: 0.53\n",
            "Epoch: 0, Batch: 2180, Loss: 0.5347591429948807, Accuracy: 50.72, Precision: 0.54, Recall: 0.59\n",
            "Epoch: 0, Batch: 2190, Loss: 0.6140495896339416, Accuracy: 52.34, Precision: 0.41, Recall: 0.37\n",
            "Epoch: 0, Batch: 2200, Loss: 0.6336836576461792, Accuracy: 50.0, Precision: 0.5, Recall: 0.29\n",
            "Epoch: 0, Batch: 2210, Loss: 0.5833898901939392, Accuracy: 50.64, Precision: 0.58, Recall: 0.54\n",
            "Epoch: 0, Batch: 2220, Loss: 0.5922827005386353, Accuracy: 52.24, Precision: 0.57, Recall: 0.66\n",
            "Epoch: 0, Batch: 2230, Loss: 0.5974955946207047, Accuracy: 50.42, Precision: 0.57, Recall: 0.53\n",
            "Epoch: 0, Batch: 2240, Loss: 0.541422215104103, Accuracy: 50.1, Precision: 0.45, Recall: 0.49\n",
            "Epoch: 0, Batch: 2250, Loss: 0.5876572459936142, Accuracy: 49.82, Precision: 0.51, Recall: 0.41\n",
            "Epoch: 0, Batch: 2260, Loss: 0.5537789732217788, Accuracy: 50.48, Precision: 0.46, Recall: 0.44\n",
            "Epoch: 0, Batch: 2270, Loss: 0.7023093581199646, Accuracy: 50.36, Precision: 0.48, Recall: 0.41\n",
            "Epoch: 0, Batch: 2280, Loss: 0.649335703253746, Accuracy: 47.58, Precision: 0.61, Recall: 0.39\n",
            "Epoch: 0, Batch: 2290, Loss: 0.6059215098619462, Accuracy: 50.22, Precision: 0.51, Recall: 0.61\n",
            "Epoch: 0, Batch: 2300, Loss: 0.6084337294101715, Accuracy: 50.0, Precision: 0.5, Recall: 0.51\n",
            "Epoch: 0, Batch: 2310, Loss: 0.6210117280483246, Accuracy: 51.52, Precision: 0.46, Recall: 0.31\n",
            "Epoch: 0, Batch: 2320, Loss: 0.5695236146450042, Accuracy: 56.72, Precision: 0.36, Recall: 0.26\n",
            "Epoch: 0, Batch: 2330, Loss: 0.5980806618928909, Accuracy: 48.8, Precision: 0.53, Recall: 0.3\n",
            "Epoch: 0, Batch: 2340, Loss: 0.611797446012497, Accuracy: 50.26, Precision: 0.51, Recall: 0.63\n",
            "Epoch: 0, Batch: 2350, Loss: 0.5621124297380448, Accuracy: 50.28, Precision: 0.49, Recall: 0.36\n",
            "Epoch: 0, Batch: 2360, Loss: 0.5821957737207413, Accuracy: 50.42, Precision: 0.57, Recall: 0.53\n",
            "Epoch: 0, Batch: 2370, Loss: 0.5280679166316986, Accuracy: 50.04, Precision: 0.52, Recall: 0.51\n",
            "Epoch: 0, Batch: 2380, Loss: 0.6039650529623032, Accuracy: 50.0, Precision: 0.53, Recall: 0.5\n",
            "Epoch: 0, Batch: 2390, Loss: 0.623212319612503, Accuracy: 49.4, Precision: 0.55, Recall: 0.44\n",
            "Epoch: 0, Batch: 2400, Loss: 0.6145153522491456, Accuracy: 50.24, Precision: 0.52, Recall: 0.56\n",
            "Epoch: 0, Batch: 2410, Loss: 0.5760619282722473, Accuracy: 50.3, Precision: 0.53, Recall: 0.55\n",
            "Epoch: 0, Batch: 2420, Loss: 0.6320670485496521, Accuracy: 49.44, Precision: 0.52, Recall: 0.36\n",
            "Epoch: 0, Batch: 2430, Loss: 0.582456985116005, Accuracy: 53.04, Precision: 0.42, Recall: 0.31\n",
            "Epoch: 0, Batch: 2440, Loss: 0.5484313130378723, Accuracy: 50.8, Precision: 0.46, Recall: 0.4\n",
            "Epoch: 0, Batch: 2450, Loss: 0.5538840621709824, Accuracy: 50.36, Precision: 0.49, Recall: 0.32\n",
            "Epoch: 0, Batch: 2460, Loss: 0.5449026674032211, Accuracy: 50.36, Precision: 0.53, Recall: 0.56\n",
            "Epoch: 0, Batch: 2470, Loss: 0.4883023023605347, Accuracy: 50.28, Precision: 0.48, Recall: 0.43\n",
            "Epoch: 0, Batch: 2480, Loss: 0.5216010481119155, Accuracy: 51.76, Precision: 0.61, Recall: 0.58\n",
            "Epoch: 0, Batch: 2490, Loss: 0.7540435791015625, Accuracy: 50.2, Precision: 0.4, Recall: 0.49\n",
            "Epoch: 0, Batch: 2500, Loss: 0.6190321564674377, Accuracy: 48.56, Precision: 0.59, Recall: 0.42\n",
            "Epoch: 0, Batch: 2510, Loss: 0.6476894736289978, Accuracy: 49.76, Precision: 0.49, Recall: 0.62\n",
            "Epoch: 0, Batch: 2520, Loss: 0.5893163651227951, Accuracy: 50.88, Precision: 0.46, Recall: 0.39\n",
            "Epoch: 0, Batch: 2530, Loss: 0.5834315896034241, Accuracy: 50.78, Precision: 0.47, Recall: 0.37\n",
            "Epoch: 0, Batch: 2540, Loss: 0.5530583202838898, Accuracy: 50.24, Precision: 0.49, Recall: 0.38\n",
            "Epoch: 0, Batch: 2550, Loss: 0.5379533559083939, Accuracy: 50.4, Precision: 0.45, Recall: 0.46\n",
            "Epoch: 0, Batch: 2560, Loss: 0.5977189391851425, Accuracy: 49.64, Precision: 0.53, Recall: 0.44\n",
            "Epoch: 0, Batch: 2570, Loss: 0.5912516981363296, Accuracy: 49.86, Precision: 0.43, Recall: 0.51\n",
            "Epoch: 0, Batch: 2580, Loss: 0.6176448464393616, Accuracy: 50.4, Precision: 0.46, Recall: 0.45\n",
            "Epoch: 0, Batch: 2590, Loss: 0.5382349967956543, Accuracy: 50.36, Precision: 0.59, Recall: 0.52\n",
            "Epoch: 0, Batch: 2600, Loss: 0.6233788698911666, Accuracy: 51.2, Precision: 0.56, Recall: 0.6\n",
            "Epoch: 0, Batch: 2610, Loss: 0.5860155105590821, Accuracy: 50.0, Precision: 0.5, Recall: 0.58\n",
            "Epoch: 0, Batch: 2620, Loss: 0.5482860714197159, Accuracy: 50.36, Precision: 0.48, Recall: 0.41\n",
            "Epoch: 0, Batch: 2630, Loss: 0.5686898618936539, Accuracy: 50.0, Precision: 0.5, Recall: 0.47\n",
            "Epoch: 0, Batch: 2640, Loss: 0.5527939587831497, Accuracy: 50.7, Precision: 0.57, Recall: 0.55\n",
            "Epoch: 0, Batch: 2650, Loss: 0.5711887508630753, Accuracy: 49.7, Precision: 0.45, Recall: 0.53\n",
            "Epoch: 0, Batch: 2660, Loss: 0.5800738990306854, Accuracy: 49.3, Precision: 0.57, Recall: 0.45\n",
            "Epoch: 0, Batch: 2670, Loss: 0.5202756315469742, Accuracy: 50.06, Precision: 0.51, Recall: 0.53\n",
            "Epoch: 0, Batch: 2680, Loss: 0.6781575322151184, Accuracy: 50.96, Precision: 0.44, Recall: 0.42\n",
            "Epoch: 0, Batch: 2690, Loss: 0.5295010209083557, Accuracy: 50.04, Precision: 0.49, Recall: 0.48\n",
            "Epoch: 0, Batch: 2700, Loss: 0.6160844534635543, Accuracy: 51.5, Precision: 0.45, Recall: 0.35\n",
            "Epoch: 0, Batch: 2710, Loss: 0.6035485357046128, Accuracy: 50.0, Precision: 0.59, Recall: 0.5\n",
            "Epoch: 0, Batch: 2720, Loss: 0.5467359244823455, Accuracy: 51.4, Precision: 0.43, Recall: 0.4\n",
            "Epoch: 0, Batch: 2730, Loss: 0.5889094442129135, Accuracy: 51.2, Precision: 0.44, Recall: 0.4\n",
            "Epoch: 0, Batch: 2740, Loss: 0.5750892519950866, Accuracy: 51.2, Precision: 0.47, Recall: 0.3\n",
            "Epoch: 0, Batch: 2750, Loss: 0.48364914357662203, Accuracy: 50.0, Precision: 0.54, Recall: 0.5\n",
            "Epoch: 0, Batch: 2760, Loss: 0.6146572291851043, Accuracy: 50.0, Precision: 0.5, Recall: 0.48\n",
            "Epoch: 0, Batch: 2770, Loss: 0.5504280656576157, Accuracy: 50.54, Precision: 0.47, Recall: 0.41\n",
            "Epoch: 0, Batch: 2780, Loss: 0.5570470780134201, Accuracy: 50.12, Precision: 0.56, Recall: 0.51\n",
            "Epoch: 0, Batch: 2790, Loss: 0.6139497935771943, Accuracy: 50.0, Precision: 0.5, Recall: 0.55\n",
            "Epoch: 0, Batch: 2800, Loss: 0.5155507951974869, Accuracy: 50.04, Precision: 0.49, Recall: 0.48\n",
            "Epoch: 0, Batch: 2810, Loss: 0.5342657476663589, Accuracy: 50.0, Precision: 0.5, Recall: 0.39\n",
            "Epoch: 0, Batch: 2820, Loss: 0.5499536037445069, Accuracy: 51.2, Precision: 0.56, Recall: 0.6\n",
            "Epoch: 0, Batch: 2830, Loss: 0.6275585025548935, Accuracy: 49.52, Precision: 0.48, Recall: 0.62\n",
            "Epoch: 0, Batch: 2840, Loss: 0.613105058670044, Accuracy: 47.9, Precision: 0.55, Recall: 0.29\n",
            "Epoch: 0, Batch: 2850, Loss: 0.5053783744573593, Accuracy: 50.0, Precision: 0.51, Recall: 0.5\n",
            "Epoch: 0, Batch: 2860, Loss: 0.561467757821083, Accuracy: 50.64, Precision: 0.46, Recall: 0.42\n",
            "Epoch: 0, Batch: 2870, Loss: 0.6496080458164215, Accuracy: 50.4, Precision: 0.49, Recall: 0.3\n",
            "Epoch: 0, Batch: 2880, Loss: 0.7163003981113434, Accuracy: 50.0, Precision: 0.55, Recall: 0.5\n",
            "Epoch: 0, Batch: 2890, Loss: 0.5200160712003707, Accuracy: 50.4, Precision: 0.52, Recall: 0.6\n",
            "Epoch: 0, Batch: 2900, Loss: 0.5506417006254196, Accuracy: 50.0, Precision: 0.5, Recall: 0.49\n",
            "Epoch: 0, Batch: 2910, Loss: 0.6179536312818528, Accuracy: 50.0, Precision: 0.56, Recall: 0.5\n",
            "Epoch: 0, Batch: 2920, Loss: 0.5633765935897828, Accuracy: 49.3, Precision: 0.45, Recall: 0.57\n",
            "Epoch: 0, Batch: 2930, Loss: 0.5238021314144135, Accuracy: 50.0, Precision: 0.56, Recall: 0.5\n",
            "Epoch: 0, Batch: 2940, Loss: 0.5731442749500275, Accuracy: 50.0, Precision: 0.5, Recall: 0.5\n",
            "Epoch: 0, Batch: 2950, Loss: 0.5141425937414169, Accuracy: 50.64, Precision: 0.58, Recall: 0.54\n",
            "Epoch: 0, Batch: 2960, Loss: 0.5834845632314682, Accuracy: 50.02, Precision: 0.49, Recall: 0.49\n",
            "Epoch: 0, Batch: 2970, Loss: 0.5684795647859573, Accuracy: 50.0, Precision: 0.5, Recall: 0.47\n",
            "Epoch: 0, Batch: 2980, Loss: 0.5423060923814773, Accuracy: 49.68, Precision: 0.54, Recall: 0.46\n",
            "Epoch: 0, Batch: 2990, Loss: 0.5375021725893021, Accuracy: 50.8, Precision: 0.55, Recall: 0.58\n",
            "Epoch: 0, Batch: 3000, Loss: 0.5835905849933625, Accuracy: 50.8, Precision: 0.4, Recall: 0.46\n",
            "Epoch: 0, Batch: 3010, Loss: 0.6375717252492905, Accuracy: 49.52, Precision: 0.53, Recall: 0.42\n",
            "Epoch: 0, Batch: 3020, Loss: 0.5430888921022415, Accuracy: 49.94, Precision: 0.47, Recall: 0.51\n",
            "Epoch: 0, Batch: 3030, Loss: 0.5787083476781845, Accuracy: 50.18, Precision: 0.49, Recall: 0.41\n",
            "Epoch: 0, Batch: 3040, Loss: 0.5695124864578247, Accuracy: 50.84, Precision: 0.47, Recall: 0.36\n",
            "Epoch: 0, Batch: 3050, Loss: 0.5504191547632218, Accuracy: 50.12, Precision: 0.51, Recall: 0.56\n"
          ]
        }
      ],
      "source": [
        "model = CNN(vocab_size,embedding_dim)\n",
        "# defining the optimizer\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "# defining the loss function\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "# checking if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    criterion = criterion.cuda()\n",
        "\n",
        "epoch_number = 0\n",
        "best_vloss = float('inf')\n",
        "\n",
        "EPOCHS = 20\n",
        "\n",
        "best_vloss = float('inf')\n",
        "train_losses = [] \n",
        "val_losses = []\n",
        "test_losses = []\n",
        "for epoch in range(EPOCHS):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "    # Make sure gradient tracking is on, and do a pass over the data\n",
        "    model.train(True)\n",
        "    avg_loss = train_epoch(epoch_number)\n",
        "    train_losses.append(avg_loss)\n",
        "\n",
        "    # Where we're going, we don't need gradients\n",
        "    model.eval() \n",
        "\n",
        "    running_vloss = 0.0\n",
        "    total_val_predictions = []\n",
        "    total_val_labels = []\n",
        "    for i, vdata in enumerate(val_loader):\n",
        "        vlabels, vinputs, offset = vdata\n",
        "        if len(vlabels) == 10:\n",
        "          vinputs = vinputs.reshape([10, 23]).to(device=device,dtype=torch.long)\n",
        "          vlabels = vlabels.float().to(device=device) \n",
        "          voutputs = model(vinputs)\n",
        "          vloss = criterion(voutputs, vlabels.unsqueeze(1))\n",
        "          running_vloss += vloss.item()\n",
        "\n",
        "          # Calculate metrics for validation set\n",
        "          rounded_vpreds = torch.round(torch.sigmoid(voutputs))\n",
        "          total_val_predictions.extend(rounded_vpreds.tolist())\n",
        "          total_val_labels.extend(vlabels.tolist())\n",
        "        else:\n",
        "          pass\n",
        "    avg_vloss = running_vloss / len(val_loader)\n",
        "    val_losses.append(avg_vloss)\n",
        "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
        "\n",
        "    val_accuracy, val_precision, val_recall = compute_metrics(torch.tensor(total_val_predictions), torch.tensor(total_val_labels))\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    val_precisions.append(val_precision)\n",
        "    val_recalls.append(val_recall)\n",
        "    # Track best performance, and save the model's state\n",
        "    if avg_vloss < best_vloss:\n",
        "        best_vloss = avg_vloss\n",
        "\n",
        "    epoch_number += 1\n",
        "\n",
        "# After all epochs, evaluate the model on the test set\n",
        "model.eval()  # Ensure the model is in evaluation mode\n",
        "running_test_loss = 0.0\n",
        "total_test_predictions = []\n",
        "total_test_labels = []\n",
        "\n",
        "for i, tdata in enumerate(test_loader):\n",
        "    tlabels, tinputs, offset = tdata\n",
        "    if len(tlabels) == 10:\n",
        "      tinputs = tinputs.reshape([10, 23]).to(device=device, dtype=torch.long)\n",
        "      tlabels = tlabels.float().to(device=device)\n",
        "      toutputs = model(tinputs)\n",
        "      tloss = criterion(toutputs, tlabels.unsqueeze(1))\n",
        "      running_test_loss += tloss.item()\n",
        "      # Calculate metrics for testn set\n",
        "      rounded_tpreds = torch.round(torch.sigmoid(toutputs))\n",
        "      total_test_predictions.extend(rounded_tpreds.tolist())\n",
        "      total_test_labels.extend(tlabels.tolist())\n",
        "\n",
        "avg_test_loss = running_test_loss / len(test_loader)\n",
        "test_losses.append(avg_test_loss)\n",
        "test_accuracy, test_precision, test_recall = compute_metrics(torch.tensor(total_test_predictions), torch.tensor(total_test_labels))\n",
        "test_accuracies.append(test_accuracy)\n",
        "test_precisions.append(test_precision)\n",
        "test_recalls.append(test_recall)\n",
        "print('Test Loss: {}'.format(avg_test_loss))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "JuP30WH9Zy0X",
        "outputId": "7ae3bb8f-da8f-45ee-c53d-633dc6c53f44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "x and y must have same first dimension, but have shapes (20,) and (0,)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[66], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, EPOCHS)\n\u001b[1;32m     10\u001b[0m \u001b[39m# Plot and label the training and validation loss values\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m plt\u001b[39m.\u001b[39;49mplot(epochs, train_losses, label\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mTraining Loss\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     12\u001b[0m plt\u001b[39m.\u001b[39mplot(epochs, val_losses, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValidation Loss\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[39m# Add in a title and axes labels\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/mlclass/lib/python3.10/site-packages/matplotlib/pyplot.py:2812\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[1;32m   2811\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, scaley\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 2812\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39;49mplot(\n\u001b[1;32m   2813\u001b[0m         \u001b[39m*\u001b[39;49margs, scalex\u001b[39m=\u001b[39;49mscalex, scaley\u001b[39m=\u001b[39;49mscaley,\n\u001b[1;32m   2814\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/mlclass/lib/python3.10/site-packages/matplotlib/axes/_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[0;32m-> 1688\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[1;32m   1689\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[1;32m   1690\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/mlclass/lib/python3.10/site-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(\n\u001b[1;32m    312\u001b[0m     this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39;49mambiguous_fmt_datakey)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/mlclass/lib/python3.10/site-packages/matplotlib/axes/_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39mupdate_units(y)\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[0;32m--> 504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have same first dimension, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhave shapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    506\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    507\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y can be no greater than 2D, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (20,) and (0,)"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Generate a sequence of integers to represent the epoch numbers\n",
        "epochs = range(1, EPOCHS+1)\n",
        "\n",
        "# Plot the training and validation loss values\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_losses, label='Training Loss')\n",
        "plt.plot(epochs, val_losses, label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "# Plot the training and validation accuracy values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, train_accuracies, label='Training Accuracy')\n",
        "plt.plot(epochs, val_accuracies, label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Print test metrics\n",
        "print(\"Test Loss: {:.4f}\".format(avg_test_loss))\n",
        "print(\"Test Accuracy: {:.4f}\".format(test_accuracy))\n",
        "print(\"Test Precision: {:.4f}\".format(test_precision))\n",
        "print(\"Test Recall: {:.4f}\".format(test_recall))\n",
        "\n",
        "# Create a confusion matrix for the test set\n",
        "test_confusion = confusion_matrix(total_test_labels, total_test_predictions)\n",
        "plt.figure(figsize=(5, 4))\n",
        "sns.heatmap(test_confusion, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix - Test Set')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOWit7ZZVzCpF+agTcHetCd",
      "include_colab_link": true,
      "mount_file_id": "1uYL8qjFTmqSwUOiGG1V4Nuut6-puTFf5",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
