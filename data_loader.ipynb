{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers for Humans vs AI Text Identification\n",
    "### CAPP 30255\n",
    "**Wesley Janson**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/data.csv')\n",
    "clean_data = data[['intro','type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv):\n",
    "  # Reads the raw csv file and split into\n",
    "  # sentences (x) and target (y)\n",
    "  df = pd.read_csv(csv)\n",
    "  \n",
    "  \n",
    "  text = df['intro'].values\n",
    "  labels = df['type'].values\n",
    "  return labels,text\n",
    "labels,text = load_data('data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function processes training data, establishing number IDs for each vocabulary word,\n",
    "# converting word sequence into ID sequence (input_as_ids), and providing dict\n",
    "# to map from word to its ID (word2id), and list to map from ID back to word (id2word)\n",
    "def process_training_data(corpus_text):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        # Create the model's vocabulary and map to unique indices\n",
    "        word2id = {}\n",
    "        id2word = []\n",
    "        list_of_inputs = []\n",
    "        for entry in corpus_text:\n",
    "            for word in entry:\n",
    "                if word not in word2id:\n",
    "                    id2word.append(word)\n",
    "                    word2id[word] = len(id2word) - 1\n",
    "\n",
    "            # Convert string of text into string of IDs in a tensor for input to model\n",
    "            input_as_ids = []\n",
    "            for word in entry.split():\n",
    "                input_as_ids.append(word2id[word])\n",
    "            list_of_inputs.append(input_as_ids)\n",
    "            # final_ids = torch.LongTensor(input_as_ids)\n",
    "\n",
    "        return list_of_inputs,word2id,id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "list_of_tokens = [tokenizer(x) for x in text]\n",
    "print(list_of_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m list_of_inputs,word2id,id2word \u001b[39m=\u001b[39m process_training_data(list_of_tokens)\n",
      "Cell \u001b[0;32mIn[17], line 18\u001b[0m, in \u001b[0;36mprocess_training_data\u001b[0;34m(corpus_text)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39m# Convert string of text into string of IDs in a tensor for input to model\u001b[39;00m\n\u001b[1;32m     17\u001b[0m input_as_ids \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 18\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m entry\u001b[39m.\u001b[39;49msplit():\n\u001b[1;32m     19\u001b[0m     input_as_ids\u001b[39m.\u001b[39mappend(word2id[word])\n\u001b[1;32m     20\u001b[0m list_of_inputs\u001b[39m.\u001b[39mappend(input_as_ids)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "\n",
    "list_of_inputs,word2id,id2word = process_training_data(list_of_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m tokenizer \u001b[39m=\u001b[39m get_tokenizer(\u001b[39m\"\u001b[39m\u001b[39mbasic_english\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39m# list_of_tokens = [tokenizer(x) for x in text]\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# print(list_of_tokens)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m token_list \u001b[39m=\u001b[39m tokenizer(text\u001b[39m.\u001b[39;49mtolist())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlclass/lib/python3.10/site-packages/torchtext/data/utils.py:46\u001b[0m, in \u001b[0;36m_basic_english_normalize\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_basic_english_normalize\u001b[39m(line):\n\u001b[1;32m     25\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39m    Basic normalization for a line of text.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m    Normalization includes\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39m    Returns a list of tokens after splitting on whitespace.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     line \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39;49mlower()\n\u001b[1;32m     47\u001b[0m     \u001b[39mfor\u001b[39;00m pattern_re, replaced_str \u001b[39min\u001b[39;00m _patterns_dict:\n\u001b[1;32m     48\u001b[0m         line \u001b[39m=\u001b[39m pattern_re\u001b[39m.\u001b[39msub(replaced_str, line)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "# list_of_tokens = [tokenizer(x) for x in text]\n",
    "# print(list_of_tokens)\n",
    "token_list = tokenizer(text.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# token_list = text.apply(tokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fdist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[39mfor\u001b[39;00m idx, word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(common_words):\n\u001b[1;32m     13\u001b[0m         vocabulary[word[\u001b[39m0\u001b[39m]] \u001b[39m=\u001b[39m (idx\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m build_vocabulary(list_of_tokens)\n",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m, in \u001b[0;36mbuild_vocabulary\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m par:\n\u001b[1;32m      8\u001b[0m         freqdist[word] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 10\u001b[0m common_words \u001b[39m=\u001b[39m fdist\u001b[39m.\u001b[39mmost_common(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_words)\n\u001b[1;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m idx, word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(common_words):\n\u001b[1;32m     13\u001b[0m     vocabulary[word[\u001b[39m0\u001b[39m]] \u001b[39m=\u001b[39m (idx\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fdist' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "def build_vocabulary(tokens):\n",
    "  # Builds the vocabulary and keeps the \"x\" most frequent words\n",
    "    vocabulary = dict()\n",
    "    freqdist = FreqDist()\n",
    "    for par in text:\n",
    "        for word in par:\n",
    "            freqdist[word] += 1\n",
    "    \n",
    "    common_words = freqdist.most_common()\n",
    "    \n",
    "    for idx, word in enumerate(common_words):\n",
    "        vocabulary[word[0]] = (idx+1)\n",
    "build_vocabulary(list_of_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/mlclass/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from torchtext.vocab import GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "~/.vector_cache/glove.6B.zip:  23%|██▎       | 195M/862M [00:34<01:57, 5.66MB/s]    \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m VECTOR_CACHE_DIR \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m~/.vector_cache\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m glove \u001b[39m=\u001b[39m GloVe(name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m6B\u001b[39;49m\u001b[39m'\u001b[39;49m, cache \u001b[39m=\u001b[39;49m VECTOR_CACHE_DIR)\n\u001b[1;32m      5\u001b[0m vecs \u001b[39m=\u001b[39m glove\u001b[39m.\u001b[39mget_vecs_by_tokens(list_of_tokens[\u001b[39m0\u001b[39m])\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(vecs\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlclass/lib/python3.10/site-packages/torchtext/vocab/vectors.py:220\u001b[0m, in \u001b[0;36mGloVe.__init__\u001b[0;34m(self, name, dim, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl[name]\n\u001b[1;32m    219\u001b[0m name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mglove.\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39md.txt\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(name, \u001b[39mstr\u001b[39m(dim))\n\u001b[0;32m--> 220\u001b[0m \u001b[39msuper\u001b[39;49m(GloVe, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(name, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlclass/lib/python3.10/site-packages/torchtext/vocab/vectors.py:59\u001b[0m, in \u001b[0;36mVectors.__init__\u001b[0;34m(self, name, cache, url, unk_init, max_vectors)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munk_init \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor\u001b[39m.\u001b[39mzero_ \u001b[39mif\u001b[39;00m unk_init \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m unk_init\n\u001b[0;32m---> 59\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache(name, cache, url\u001b[39m=\u001b[39;49murl, max_vectors\u001b[39m=\u001b[39;49mmax_vectors)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlclass/lib/python3.10/site-packages/torchtext/vocab/vectors.py:98\u001b[0m, in \u001b[0;36mVectors.cache\u001b[0;34m(self, name, cache, url, max_vectors)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# remove the partial zip file\u001b[39;00m\n\u001b[1;32m     97\u001b[0m             os\u001b[39m.\u001b[39mremove(dest)\n\u001b[0;32m---> 98\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     99\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mExtracting vectors into \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(cache))\n\u001b[1;32m    100\u001b[0m ext \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msplitext(dest)[\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlclass/lib/python3.10/site-packages/torchtext/vocab/vectors.py:95\u001b[0m, in \u001b[0;36mVectors.cache\u001b[0;34m(self, name, cache, url, max_vectors)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m, unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, miniters\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, desc\u001b[39m=\u001b[39mdest) \u001b[39mas\u001b[39;00m t:\n\u001b[1;32m     94\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m         urlretrieve(url, dest, reporthook\u001b[39m=\u001b[39;49mreporthook(t))\n\u001b[1;32m     96\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# remove the partial zip file\u001b[39;00m\n\u001b[1;32m     97\u001b[0m         os\u001b[39m.\u001b[39mremove(dest)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlclass/lib/python3.10/urllib/request.py:270\u001b[0m, in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    267\u001b[0m     reporthook(blocknum, bs, size)\n\u001b[1;32m    269\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     block \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39;49mread(bs)\n\u001b[1;32m    271\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m block:\n\u001b[1;32m    272\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlclass/lib/python3.10/http/client.py:465\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[1;32m    463\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    464\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[0;32m--> 465\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(amt)\n\u001b[1;32m    466\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[1;32m    467\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlclass/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlclass/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlclass/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "VECTOR_CACHE_DIR = '~/.vector_cache'\n",
    "\n",
    "glove = GloVe(name='6B', cache = VECTOR_CACHE_DIR)\n",
    "\n",
    "vecs = glove.get_vecs_by_tokens(list_of_tokens[0])\n",
    "\n",
    "print(vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
